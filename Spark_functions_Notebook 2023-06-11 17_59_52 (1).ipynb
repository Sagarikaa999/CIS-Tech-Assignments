{"cells":[{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.functions import to_timestamp, current_timestamp\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\nfrom pyspark.sql.functions import current_date\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nschema = StructType([\n            StructField(\"seq\", StringType(), True)])\n\ndates = ['1']\n\ncurrent_timestamp_df = spark.range(1).select(current_timestamp().alias('current_timestamp'))\n\n# Show the current timestamp\ncurrent_timestamp_df.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0056ae6a-88ed-4878-86cd-3080d54c1ddd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------------------+\n|current_timestamp      |\n+-----------------------+\n|2023-06-18 14:58:12.652|\n+-----------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pandas as pd    \ndata = [['Scott', 50], ['Jeff', 45], ['Thomas', 54],['Ann',34]] \n  \n# Create the pandas DataFrame \npandasDF = pd.DataFrame(data, columns = ['Name', 'Age']) \n  \n# print dataframe. \nprint(pandasDF)\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .master(\"local[1]\") \\\n    .appName(\"SparkByExamples.com\") \\\n    .getOrCreate()\n\nsparkDF=spark.createDataFrame(pandasDF) \nsparkDF.printSchema()\nsparkDF.show()\n\n#sparkDF=spark.createDataFrame(pandasDF.astype(str)) \nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\nmySchema = StructType([ StructField(\"First Name\", StringType(), True)\\\n                       ,StructField(\"Age\", IntegerType(), True)])\n\nsparkDF2 = spark.createDataFrame(pandasDF,schema=mySchema)\nsparkDF2.printSchema()\nsparkDF2.show()\n\n\nspark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")\nspark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\",\"true\")\n\npandasDF2=sparkDF2.select(\"*\").toPandas\nprint(pandasDF2)\n\n\ntest=spark.conf.get(\"spark.sql.execution.arrow.enabled\")\nprint(test)\n\ntest123=spark.conf.get(\"spark.sql.execution.arrow.pyspark.fallback.enabled\")\nprint(test123)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4f9eac9e-3ed3-448f-9cab-f4da8f7f7d11","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/plain":[],"application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}"}},{"output_type":"stream","output_type":"stream","name":"stdout","text":["     Name  Age\n0   Scott   50\n1    Jeff   45\n2  Thomas   54\n3     Ann   34\nroot\n |-- Name: string (nullable = true)\n |-- Age: long (nullable = true)\n\n+------+---+\n|  Name|Age|\n+------+---+\n| Scott| 50|\n|  Jeff| 45|\n|Thomas| 54|\n|   Ann| 34|\n+------+---+\n\nroot\n |-- First Name: string (nullable = true)\n |-- Age: integer (nullable = true)\n\n+----------+---+\n|First Name|Age|\n+----------+---+\n|     Scott| 50|\n|      Jeff| 45|\n|    Thomas| 54|\n|       Ann| 34|\n+----------+---+\n\n<bound method PandasConversionMixin.toPandas of DataFrame[First Name: string, Age: int]>\ntrue\ntrue\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nfrom pyspark.sql.functions import col,expr\ndata=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)]\nspark.createDataFrame(data).toDF(\"date\",\"increment\") \\\n    .select(col(\"date\"),col(\"increment\"), \\\n      expr(\"add_months(to_date(date,'yyyy-MM-dd'),cast(increment as int))\").alias(\"inc_date\")) \\\n    .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"89f7c8c9-8e0e-4dd1-b96d-82b7ff0b5560","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+---------+----------+\n|      date|increment|  inc_date|\n+----------+---------+----------+\n|2019-01-23|        1|2019-02-23|\n|2019-06-24|        2|2019-08-24|\n|2019-09-20|        3|2019-12-20|\n+----------+---------+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\ndata = [('James','Smith','M',3000),\n  ('Anna','Rose','F',4100),\n  ('Robert','Williams','M',6200), \n]\n\ncolumns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\ndf = spark.createDataFrame(data=data, schema = columns)\ndf.show()\n\n\nif 'salary1' not in df.columns:\n    print(\"aa\")\n    \n# Add new constanct column\nfrom pyspark.sql.functions import lit\ndf.withColumn(\"bonus_percent\", lit(0.3)) \\\n  .show()\n  \n#Add column from existing column\ndf.withColumn(\"bonus_amount\", df.salary*0.3) \\\n  .show()\n\n#Add column by concatinating existing columns\nfrom pyspark.sql.functions import concat_ws\ndf.withColumn(\"name\", concat_ws(\",\",\"firstname\",'lastname')) \\\n  .show()\n\n#Add current date\nfrom pyspark.sql.functions import current_date\ndf.withColumn(\"current_date\", current_date()) \\\n  .show()\n\n\nfrom pyspark.sql.functions import when\ndf.withColumn(\"grade\", \\\n   when((df.salary < 4000), lit(\"A\")) \\\n     .when((df.salary >= 4000) & (df.salary <= 5000), lit(\"B\")) \\\n     .otherwise(lit(\"C\")) \\\n  ).show()\n    \n# Add column using select\ndf.select(\"firstname\",\"salary\", lit(0.3).alias(\"bonus\")).show()\ndf.select(\"firstname\",\"salary\", lit(df.salary * 0.3).alias(\"bonus_amount\")).show()\ndf.select(\"firstname\",\"salary\", current_date().alias(\"today_date\")).show()\n\n#Add columns using SQL\ndf.createOrReplaceTempView(\"PER\")\nspark.sql(\"select firstname,salary, '0.3' as bonus from PER\").show()\nspark.sql(\"select firstname,salary, salary * 0.3 as bonus_amount from PER\").show()\nspark.sql(\"select firstname,salary, current_date() as today_date from PER\").show()\nspark.sql(\"select firstname,salary, \" +\n          \"case salary when salary < 4000 then 'A' \"+\n          \"else 'B' END as grade from PER\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"f01a32b0-ee81-4566-bdae-f62baae0d1e5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+--------+------+------+\n|firstname|lastname|gender|salary|\n+---------+--------+------+------+\n|    James|   Smith|     M|  3000|\n|     Anna|    Rose|     F|  4100|\n|   Robert|Williams|     M|  6200|\n+---------+--------+------+------+\n\naa\n+---------+--------+------+------+-------------+\n|firstname|lastname|gender|salary|bonus_percent|\n+---------+--------+------+------+-------------+\n|    James|   Smith|     M|  3000|          0.3|\n|     Anna|    Rose|     F|  4100|          0.3|\n|   Robert|Williams|     M|  6200|          0.3|\n+---------+--------+------+------+-------------+\n\n+---------+--------+------+------+------------+\n|firstname|lastname|gender|salary|bonus_amount|\n+---------+--------+------+------+------------+\n|    James|   Smith|     M|  3000|       900.0|\n|     Anna|    Rose|     F|  4100|      1230.0|\n|   Robert|Williams|     M|  6200|      1860.0|\n+---------+--------+------+------+------------+\n\n+---------+--------+------+------+---------------+\n|firstname|lastname|gender|salary|           name|\n+---------+--------+------+------+---------------+\n|    James|   Smith|     M|  3000|    James,Smith|\n|     Anna|    Rose|     F|  4100|      Anna,Rose|\n|   Robert|Williams|     M|  6200|Robert,Williams|\n+---------+--------+------+------+---------------+\n\n+---------+--------+------+------+------------+\n|firstname|lastname|gender|salary|current_date|\n+---------+--------+------+------+------------+\n|    James|   Smith|     M|  3000|  2023-06-18|\n|     Anna|    Rose|     F|  4100|  2023-06-18|\n|   Robert|Williams|     M|  6200|  2023-06-18|\n+---------+--------+------+------+------------+\n\n+---------+--------+------+------+-----+\n|firstname|lastname|gender|salary|grade|\n+---------+--------+------+------+-----+\n|    James|   Smith|     M|  3000|    A|\n|     Anna|    Rose|     F|  4100|    B|\n|   Robert|Williams|     M|  6200|    C|\n+---------+--------+------+------+-----+\n\n+---------+------+-----+\n|firstname|salary|bonus|\n+---------+------+-----+\n|    James|  3000|  0.3|\n|     Anna|  4100|  0.3|\n|   Robert|  6200|  0.3|\n+---------+------+-----+\n\n+---------+------+------------+\n|firstname|salary|bonus_amount|\n+---------+------+------------+\n|    James|  3000|       900.0|\n|     Anna|  4100|      1230.0|\n|   Robert|  6200|      1860.0|\n+---------+------+------------+\n\n+---------+------+----------+\n|firstname|salary|today_date|\n+---------+------+----------+\n|    James|  3000|2023-06-18|\n|     Anna|  4100|2023-06-18|\n|   Robert|  6200|2023-06-18|\n+---------+------+----------+\n\n+---------+------+-----+\n|firstname|salary|bonus|\n+---------+------+-----+\n|    James|  3000|  0.3|\n|     Anna|  4100|  0.3|\n|   Robert|  6200|  0.3|\n+---------+------+-----+\n\n+---------+------+------------+\n|firstname|salary|bonus_amount|\n+---------+------+------------+\n|    James|  3000|       900.0|\n|     Anna|  4100|      1230.0|\n|   Robert|  6200|      1860.0|\n+---------+------+------------+\n\n+---------+------+----------+\n|firstname|salary|today_date|\n+---------+------+----------+\n|    James|  3000|2023-06-18|\n|     Anna|  4100|2023-06-18|\n|   Robert|  6200|2023-06-18|\n+---------+------+----------+\n\n+---------+------+-----+\n|firstname|salary|grade|\n+---------+------+-----+\n|    James|  3000|    B|\n|     Anna|  4100|    B|\n|   Robert|  6200|    B|\n+---------+------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import approx_count_distinct,collect_list\nfrom pyspark.sql.functions import collect_set,sum,avg,max,countDistinct,count\nfrom pyspark.sql.functions import first, last, kurtosis, min, mean, skewness \nfrom pyspark.sql.functions import stddev, stddev_samp, stddev_pop, sumDistinct\nfrom pyspark.sql.functions import variance,var_samp,  var_pop\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nsimpleData = [(\"James\", \"Sales\", 3000),\n    (\"Michael\", \"Sales\", 4600),\n    (\"Robert\", \"Sales\", 4100),\n    (\"Maria\", \"Finance\", 3000),\n    (\"James\", \"Sales\", 3000),\n    (\"Scott\", \"Finance\", 3300),\n    (\"Jen\", \"Finance\", 3900),\n    (\"Jeff\", \"Marketing\", 3000),\n    (\"Kumar\", \"Marketing\", 2000),\n    (\"Saif\", \"Sales\", 4100)\n  ]\nschema = [\"employee_name\", \"department\", \"salary\"]\n  \n  \ndf = spark.createDataFrame(data=simpleData, schema = schema)\ndf.printSchema()\ndf.show(truncate=False)\n\nprint(\"approx_count_distinct: \" + \\\n      str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))\n\nprint(\"avg: \" + str(df.select(avg(\"salary\")).collect()[0][0]))\n\ndf.select(collect_list(\"salary\")).show(truncate=False)\n\ndf.select(collect_set(\"salary\")).show(truncate=False)\n\ndf2 = df.select(countDistinct(\"department\", \"salary\"))\ndf2.show(truncate=False)\nprint(\"Distinct Count of Department &amp; Salary: \"+str(df2.collect()[0][0]))\n\nprint(\"count: \"+str(df.select(count(\"salary\")).collect()[0]))\ndf.select(first(\"salary\")).show(truncate=False)\ndf.select(last(\"salary\")).show(truncate=False)\ndf.select(kurtosis(\"salary\")).show(truncate=False)\ndf.select(max(\"salary\")).show(truncate=False)\ndf.select(min(\"salary\")).show(truncate=False)\ndf.select(mean(\"salary\")).show(truncate=False)\ndf.select(skewness(\"salary\")).show(truncate=False)\ndf.select(stddev(\"salary\"), stddev_samp(\"salary\"), \\\n    stddev_pop(\"salary\")).show(truncate=False)\ndf.select(sum(\"salary\")).show(truncate=False)\ndf.select(sumDistinct(\"salary\")).show(truncate=False)\ndf.select(variance(\"salary\"),var_samp(\"salary\"),var_pop(\"salary\")) \\\n  .show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d1023cca-a5b9-447c-a172-a6ab3459a5e3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\napprox_count_distinct: 6\navg: 3400.0\n+------------------------------------------------------------+\n|collect_list(salary)                                        |\n+------------------------------------------------------------+\n|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n+------------------------------------------------------------+\n\n+------------------------------------+\n|collect_set(salary)                 |\n+------------------------------------+\n|[4600, 3000, 3900, 4100, 3300, 2000]|\n+------------------------------------+\n\n+----------------------------------+\n|count(DISTINCT department, salary)|\n+----------------------------------+\n|8                                 |\n+----------------------------------+\n\nDistinct Count of Department &amp; Salary: 8\ncount: Row(count(salary)=10)\n+-------------+\n|first(salary)|\n+-------------+\n|3000         |\n+-------------+\n\n+------------+\n|last(salary)|\n+------------+\n|4100        |\n+------------+\n\n+-------------------+\n|kurtosis(salary)   |\n+-------------------+\n|-0.6467803030303032|\n+-------------------+\n\n+-----------+\n|max(salary)|\n+-----------+\n|4600       |\n+-----------+\n\n+-----------+\n|min(salary)|\n+-----------+\n|2000       |\n+-----------+\n\n+-----------+\n|avg(salary)|\n+-----------+\n|3400.0     |\n+-----------+\n\n+--------------------+\n|skewness(salary)    |\n+--------------------+\n|-0.12041791181069571|\n+--------------------+\n\n+-------------------+-------------------+------------------+\n|stddev_samp(salary)|stddev_samp(salary)|stddev_pop(salary)|\n+-------------------+-------------------+------------------+\n|765.9416862050705  |765.9416862050705  |726.636084983398  |\n+-------------------+-------------------+------------------+\n\n+-----------+\n|sum(salary)|\n+-----------+\n|34000      |\n+-----------+\n\n"]},{"output_type":"stream","output_type":"stream","name":"stderr","text":["/databricks/spark/python/pyspark/sql/functions.py:723: FutureWarning: Deprecated in 3.2, use sum_distinct instead.\n  warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\n"]},{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+\n|sum(DISTINCT salary)|\n+--------------------+\n|20900               |\n+--------------------+\n\n+-----------------+-----------------+---------------+\n|var_samp(salary) |var_samp(salary) |var_pop(salary)|\n+-----------------+-----------------+---------------+\n|586666.6666666666|586666.6666666666|528000.0       |\n+-----------------+-----------------+---------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[1]\") \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\ncolumns = [\"name\",\"languagesAtSchool\",\"currentState\"]\ndata = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n\ndf = spark.createDataFrame(data=data,schema=columns)\ndf.printSchema()\ndf.show(truncate=False)\n\nfrom pyspark.sql.functions import col, concat_ws\ndf2 = df.withColumn(\"languagesAtSchool\",\n   concat_ws(\",\",col(\"languagesAtSchool\")))\ndf2.printSchema()\ndf2.show(truncate=False)\n\n\ndf.createOrReplaceTempView(\"ARRAY_STRING\")\nspark.sql(\"select name, concat_ws(',',languagesAtSchool) as languagesAtSchool,currentState from ARRAY_STRING\").show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a9df7313-7feb-4ac3-bec0-8d211ddd7ca6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- languagesAtSchool: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- currentState: string (nullable = true)\n\n+----------------+------------------+------------+\n|name            |languagesAtSchool |currentState|\n+----------------+------------------+------------+\n|James,,Smith    |[Java, Scala, C++]|CA          |\n|Michael,Rose,   |[Spark, Java, C++]|NJ          |\n|Robert,,Williams|[CSharp, VB]      |NV          |\n+----------------+------------------+------------+\n\nroot\n |-- name: string (nullable = true)\n |-- languagesAtSchool: string (nullable = false)\n |-- currentState: string (nullable = true)\n\n+----------------+-----------------+------------+\n|name            |languagesAtSchool|currentState|\n+----------------+-----------------+------------+\n|James,,Smith    |Java,Scala,C++   |CA          |\n|Michael,Rose,   |Spark,Java,C++   |NJ          |\n|Robert,,Williams|CSharp,VB        |NV          |\n+----------------+-----------------+------------+\n\n+----------------+-----------------+------------+\n|name            |languagesAtSchool|currentState|\n+----------------+-----------------+------------+\n|James,,Smith    |Java,Scala,C++   |CA          |\n|Michael,Rose,   |Spark,Java,C++   |NJ          |\n|Robert,,Williams|CSharp,VB        |NV          |\n+----------------+-----------------+------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StringType, ArrayType,StructType,StructField\nspark = SparkSession.builder \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\n\narrayCol = ArrayType(StringType(),False)\n\ndata = [\n (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"],\"OH\",\"CA\"),\n (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"],\"NY\",\"NJ\"),\n (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"],\"UT\",\"NV\")\n]\n\nschema = StructType([ \n    StructField(\"name\",StringType(),True), \n    StructField(\"languagesAtSchool\",ArrayType(StringType()),True), \n    StructField(\"languagesAtWork\",ArrayType(StringType()),True), \n    StructField(\"currentState\", StringType(), True), \n    StructField(\"previousState\", StringType(), True) \n  ])\n\ndf = spark.createDataFrame(data=data,schema=schema)\ndf.printSchema()\ndf.show()\n\nfrom pyspark.sql.functions import explode\ndf.select(df.name,explode(df.languagesAtSchool)).show()\n\nfrom pyspark.sql.functions import split\ndf.select(split(df.name,\",\").alias(\"nameAsArray\")).show()\n\nfrom pyspark.sql.functions import array\ndf.select(df.name,array(df.currentState,df.previousState).alias(\"States\")).show()\n\nfrom pyspark.sql.functions import array_contains\ndf.select(df.name,array_contains(df.languagesAtSchool,\"Java\")\n    .alias(\"array_contains\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1da201a5-2496-4567-afd9-8e13e4ca3685","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- languagesAtSchool: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- languagesAtWork: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- currentState: string (nullable = true)\n |-- previousState: string (nullable = true)\n\n+----------------+------------------+---------------+------------+-------------+\n|            name| languagesAtSchool|languagesAtWork|currentState|previousState|\n+----------------+------------------+---------------+------------+-------------+\n|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|          OH|           CA|\n|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|          NY|           NJ|\n|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|          UT|           NV|\n+----------------+------------------+---------------+------------+-------------+\n\n+----------------+------+\n|            name|   col|\n+----------------+------+\n|    James,,Smith|  Java|\n|    James,,Smith| Scala|\n|    James,,Smith|   C++|\n|   Michael,Rose,| Spark|\n|   Michael,Rose,|  Java|\n|   Michael,Rose,|   C++|\n|Robert,,Williams|CSharp|\n|Robert,,Williams|    VB|\n+----------------+------+\n\n+--------------------+\n|         nameAsArray|\n+--------------------+\n|    [James, , Smith]|\n|   [Michael, Rose, ]|\n|[Robert, , Williams]|\n+--------------------+\n\n+----------------+--------+\n|            name|  States|\n+----------------+--------+\n|    James,,Smith|[OH, CA]|\n|   Michael,Rose,|[NY, NJ]|\n|Robert,,Williams|[UT, NV]|\n+----------------+--------+\n\n+----------------+--------------+\n|            name|array_contains|\n+----------------+--------------+\n|    James,,Smith|          true|\n|   Michael,Rose,|          true|\n|Robert,,Williams|         false|\n+----------------+--------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\n\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nstates = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\nbroadcastStates = spark.sparkContext.broadcast(states)\n\ndata = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n  ]\n\ncolumns = [\"firstname\",\"lastname\",\"country\",\"state\"]\ndf = spark.createDataFrame(data = data, schema = columns)\ndf.printSchema()\ndf.show(truncate=False)\n\ndef state_convert(code):\n    return broadcastStates.value[code]\n\nresult = df.rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\nresult.show(truncate=False)\n\n# Broadcast variable on filter\n\nfilteredDf = df.where(col('state').isin([str(state) for state in broadcastStates.value]))\nfilteredDf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"7808d6bf-e69d-44bf-8f68-722997913201","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- firstname: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- country: string (nullable = true)\n |-- state: string (nullable = true)\n\n+---------+--------+-------+-----+\n|firstname|lastname|country|state|\n+---------+--------+-------+-----+\n|James    |Smith   |USA    |CA   |\n|Michael  |Rose    |USA    |NY   |\n|Robert   |Williams|USA    |CA   |\n|Maria    |Jones   |USA    |FL   |\n+---------+--------+-------+-----+\n\n+---------+--------+-------+----------+\n|firstname|lastname|country|state     |\n+---------+--------+-------+----------+\n|James    |Smith   |USA    |California|\n|Michael  |Rose    |USA    |New York  |\n|Robert   |Williams|USA    |California|\n|Maria    |Jones   |USA    |Florida   |\n+---------+--------+-------+----------+\n\n+---------+--------+-------+-----+\n|firstname|lastname|country|state|\n+---------+--------+-------+-----+\n|James    |Smith   |USA    |CA   |\n|Michael  |Rose    |USA    |NY   |\n|Robert   |Williams|USA    |CA   |\n|Maria    |Jones   |USA    |FL   |\n+---------+--------+-------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nsimpleData = [(\"James\",34,\"2006-01-01\",\"true\",\"M\",3000.60),\n    (\"Michael\",33,\"1980-01-10\",\"true\",\"F\",3300.80),\n    (\"Robert\",37,\"06-01-1992\",\"false\",\"M\",5000.50)\n  ]\n\ncolumns = [\"firstname\",\"age\",\"jobStartDate\",\"isGraduated\",\"gender\",\"salary\"]\ndf = spark.createDataFrame(data = simpleData, schema = columns)\ndf.printSchema()\ndf.show(truncate=False)\n\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import StringType,BooleanType,DateType\ndf2 = df.withColumn(\"age\",col(\"age\").cast(StringType())) \\\n    .withColumn(\"isGraduated\",col(\"isGraduated\").cast(BooleanType())) \\\n    .withColumn(\"jobStartDate\",col(\"jobStartDate\").cast(DateType()))\ndf2.printSchema()\n\ndf3 = df2.selectExpr(\"cast(age as int) age\",\n    \"cast(isGraduated as string) isGraduated\",\n    \"cast(jobStartDate as string) jobStartDate\")\ndf3.printSchema()\ndf3.show(truncate=False)\n\ndf3.createOrReplaceTempView(\"CastExample\")\ndf4 = spark.sql(\"SELECT STRING(age),BOOLEAN(isGraduated),DATE(jobStartDate) from CastExample\")\ndf4.printSchema()\ndf4.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6980d6d2-3ced-41ac-8f7a-39e00db3bad9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- firstname: string (nullable = true)\n |-- age: long (nullable = true)\n |-- jobStartDate: string (nullable = true)\n |-- isGraduated: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: double (nullable = true)\n\n+---------+---+------------+-----------+------+------+\n|firstname|age|jobStartDate|isGraduated|gender|salary|\n+---------+---+------------+-----------+------+------+\n|James    |34 |2006-01-01  |true       |M     |3000.6|\n|Michael  |33 |1980-01-10  |true       |F     |3300.8|\n|Robert   |37 |06-01-1992  |false      |M     |5000.5|\n+---------+---+------------+-----------+------+------+\n\nroot\n |-- firstname: string (nullable = true)\n |-- age: string (nullable = true)\n |-- jobStartDate: date (nullable = true)\n |-- isGraduated: boolean (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: double (nullable = true)\n\nroot\n |-- age: integer (nullable = true)\n |-- isGraduated: string (nullable = true)\n |-- jobStartDate: string (nullable = true)\n\n+---+-----------+------------+\n|age|isGraduated|jobStartDate|\n+---+-----------+------------+\n|34 |true       |2006-01-01  |\n|33 |true       |1980-01-10  |\n|37 |false      |null        |\n+---+-----------+------------+\n\nroot\n |-- age: string (nullable = true)\n |-- isGraduated: boolean (nullable = true)\n |-- jobStartDate: date (nullable = true)\n\n+---+-----------+------------+\n|age|isGraduated|jobStartDate|\n+---+-----------+------------+\n|34 |true       |2006-01-01  |\n|33 |true       |1980-01-10  |\n|37 |false      |null        |\n+---+-----------+------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.types import DoubleType, IntegerType\n# Create SparkSession\nspark = SparkSession.builder \\\n          .appName('SparkByExamples.com') \\\n          .getOrCreate()\n\nsimpleData = [(\"James\",\"34\",\"true\",\"M\",\"3000.6089\"),\n    (\"Michael\",\"33\",\"true\",\"F\",\"3300.8067\"),\n    (\"Robert\",\"37\",\"false\",\"M\",\"5000.5034\")\n  ]\n\ncolumns = [\"firstname\",\"age\",\"isGraduated\",\"gender\",\"salary\"]\ndf = spark.createDataFrame(data = simpleData, schema = columns)\ndf.printSchema()\ndf.show(truncate=False)\n\nfrom pyspark.sql.functions import col,round,expr\ndf.withColumn(\"salary\",df.salary.cast('double')).printSchema()    \ndf.withColumn(\"salary\",df.salary.cast(DoubleType())).printSchema()    \ndf.withColumn(\"salary\",col(\"salary\").cast('double')).printSchema()    \n\n#df.withColumn(\"salary\",round(df.salary.cast(DoubleType()),2)).show(truncate=False).printSchema()    \ndf.selectExpr(\"firstname\",\"isGraduated\",\"cast(salary as double) salary\").printSchema()    \n\ndf.createOrReplaceTempView(\"CastExample\")\nspark.sql(\"SELECT firstname,isGraduated,DOUBLE(salary) as salary from CastExample\").printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"77e0f5ec-3afd-4e93-b6d2-c860b8169f36","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- firstname: string (nullable = true)\n |-- age: string (nullable = true)\n |-- isGraduated: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: string (nullable = true)\n\n+---------+---+-----------+------+---------+\n|firstname|age|isGraduated|gender|salary   |\n+---------+---+-----------+------+---------+\n|James    |34 |true       |M     |3000.6089|\n|Michael  |33 |true       |F     |3300.8067|\n|Robert   |37 |false      |M     |5000.5034|\n+---------+---+-----------+------+---------+\n\nroot\n |-- firstname: string (nullable = true)\n |-- age: string (nullable = true)\n |-- isGraduated: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: double (nullable = true)\n\nroot\n |-- firstname: string (nullable = true)\n |-- age: string (nullable = true)\n |-- isGraduated: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: double (nullable = true)\n\nroot\n |-- firstname: string (nullable = true)\n |-- age: string (nullable = true)\n |-- isGraduated: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: double (nullable = true)\n\nroot\n |-- firstname: string (nullable = true)\n |-- isGraduated: string (nullable = true)\n |-- salary: double (nullable = true)\n\nroot\n |-- firstname: string (nullable = true)\n |-- isGraduated: string (nullable = true)\n |-- salary: double (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndept = [(\"Finance\",10), \\\n    (\"Marketing\",20), \\\n    (\"Sales\",30), \\\n    (\"IT\",40) \\\n  ]\ndeptColumns = [\"dept_name\",\"dept_id\"]\ndeptDF = spark.createDataFrame(data=dept, schema = deptColumns)\ndeptDF.printSchema()\ndeptDF.show(truncate=False)\n\ndataCollect = deptDF.collect()\n\nprint(dataCollect)\n\ndataCollect2 = deptDF.select(\"dept_name\").collect()\nprint(dataCollect2)\n\nfor row in dataCollect:\n    print(row['dept_name'] + \",\" +str(row['dept_id']))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"557fc5db-eebf-478c-99eb-d3039299a034","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n[Row(dept_name='Finance', dept_id=10), Row(dept_name='Marketing', dept_id=20), Row(dept_name='Sales', dept_id=30), Row(dept_name='IT', dept_id=40)]\n[Row(dept_name='Finance'), Row(dept_name='Marketing'), Row(dept_name='Sales'), Row(dept_name='IT')]\nFinance,10\nMarketing,20\nSales,30\nIT,40\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndata=[(\"James\",\"Bond\",\"100\",None),\n      (\"Ann\",\"Varsa\",\"200\",'F'),\n      (\"Tom Cruise\",\"XXX\",\"400\",''),\n      (\"Tom Brand\",None,\"400\",'M')] \ncolumns=[\"fname\",\"lname\",\"id\",\"gender\"]\ndf=spark.createDataFrame(data,columns)\n\n#alias\nfrom pyspark.sql.functions import expr\ndf.select(df.fname.alias(\"first_name\"), \\\n          df.lname.alias(\"last_name\"), \\\n          expr(\" fname ||','|| lname\").alias(\"fullName\") \\\n   ).show()\n\n#asc, desc\ndf.sort(df.fname.asc()).show()\ndf.sort(df.fname.desc()).show()\n\n#cast\ndf.select(df.fname,df.id.cast(\"int\")).printSchema()\n\n#between\ndf.filter(df.id.between(100,300)).show()\n\n#contains\ndf.filter(df.fname.contains(\"Cruise\")).show()\n\n#startswith, endswith()\ndf.filter(df.fname.startswith(\"T\")).show()\ndf.filter(df.fname.endswith(\"Cruise\")).show()\n\n#eqNullSafe\n\n#isNull & isNotNull\ndf.filter(df.lname.isNull()).show()\ndf.filter(df.lname.isNotNull()).show()\n\n#like , rlike\ndf.select(df.fname,df.lname,df.id) \\\n  .filter(df.fname.like(\"%om\")) \n\n#over\n\n#substr\ndf.select(df.fname.substr(1,2).alias(\"substr\")).show()\n\n#when & otherwise\nfrom pyspark.sql.functions import when\ndf.select(df.fname,df.lname,when(df.gender==\"M\",\"Male\") \\\n              .when(df.gender==\"F\",\"Female\") \\\n              .when(df.gender==None ,\"\") \\\n              .otherwise(df.gender).alias(\"new_gender\") \\\n    ).show()\n\n#isin\nli=[\"100\",\"200\"]\ndf.select(df.fname,df.lname,df.id) \\\n  .filter(df.id.isin(li)) \\\n  .show()\n\nfrom pyspark.sql.types import StructType,StructField,StringType,ArrayType,MapType\ndata=[((\"James\",\"Bond\"),[\"Java\",\"C#\"],{'hair':'black','eye':'brown'}),\n      ((\"Ann\",\"Varsa\"),[\".NET\",\"Python\"],{'hair':'brown','eye':'black'}),\n      ((\"Tom Cruise\",\"\"),[\"Python\",\"Scala\"],{'hair':'red','eye':'grey'}),\n      ((\"Tom Brand\",None),[\"Perl\",\"Ruby\"],{'hair':'black','eye':'blue'})]\n\nschema = StructType([\n        StructField('name', StructType([\n            StructField('fname', StringType(), True),\n            StructField('lname', StringType(), True)])),\n        StructField('languages', ArrayType(StringType()),True),\n        StructField('properties', MapType(StringType(),StringType()),True)\n     ])\ndf=spark.createDataFrame(data,schema)\ndf.printSchema()\n#getItem()\ndf.select(df.languages.getItem(1)).show()\n\ndf.select(df.properties.getItem(\"hair\")).show()\n\n#getField from Struct or Map\ndf.select(df.properties.getField(\"hair\")).show()\n\ndf.select(df.name.getField(\"fname\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"98c26050-7da0-4622-b1d9-07e20888f839","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+---------+--------------+\n|first_name|last_name|      fullName|\n+----------+---------+--------------+\n|     James|     Bond|    James,Bond|\n|       Ann|    Varsa|     Ann,Varsa|\n|Tom Cruise|      XXX|Tom Cruise,XXX|\n| Tom Brand|     null|          null|\n+----------+---------+--------------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|       Ann|Varsa|200|     F|\n|     James| Bond|100|  null|\n| Tom Brand| null|400|     M|\n|Tom Cruise|  XXX|400|      |\n+----------+-----+---+------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|Tom Cruise|  XXX|400|      |\n| Tom Brand| null|400|     M|\n|     James| Bond|100|  null|\n|       Ann|Varsa|200|     F|\n+----------+-----+---+------+\n\nroot\n |-- fname: string (nullable = true)\n |-- id: integer (nullable = true)\n\n+-----+-----+---+------+\n|fname|lname| id|gender|\n+-----+-----+---+------+\n|James| Bond|100|  null|\n|  Ann|Varsa|200|     F|\n+-----+-----+---+------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|Tom Cruise|  XXX|400|      |\n+----------+-----+---+------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|Tom Cruise|  XXX|400|      |\n| Tom Brand| null|400|     M|\n+----------+-----+---+------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|Tom Cruise|  XXX|400|      |\n+----------+-----+---+------+\n\n+---------+-----+---+------+\n|    fname|lname| id|gender|\n+---------+-----+---+------+\n|Tom Brand| null|400|     M|\n+---------+-----+---+------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|     James| Bond|100|  null|\n|       Ann|Varsa|200|     F|\n|Tom Cruise|  XXX|400|      |\n+----------+-----+---+------+\n\n+------+\n|substr|\n+------+\n|    Ja|\n|    An|\n|    To|\n|    To|\n+------+\n\n+----------+-----+----------+\n|     fname|lname|new_gender|\n+----------+-----+----------+\n|     James| Bond|      null|\n|       Ann|Varsa|    Female|\n|Tom Cruise|  XXX|          |\n| Tom Brand| null|      Male|\n+----------+-----+----------+\n\n+-----+-----+---+\n|fname|lname| id|\n+-----+-----+---+\n|James| Bond|100|\n|  Ann|Varsa|200|\n+-----+-----+---+\n\nroot\n |-- name: struct (nullable = true)\n |    |-- fname: string (nullable = true)\n |    |-- lname: string (nullable = true)\n |-- languages: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+------------+\n|languages[1]|\n+------------+\n|          C#|\n|      Python|\n|       Scala|\n|        Ruby|\n+------------+\n\n+----------------+\n|properties[hair]|\n+----------------+\n|           black|\n|           brown|\n|             red|\n|           black|\n+----------------+\n\n+----------------+\n|properties[hair]|\n+----------------+\n|           black|\n|           brown|\n|             red|\n|           black|\n+----------------+\n\n+----------+\n|name.fname|\n+----------+\n|     James|\n|       Ann|\n|Tom Cruise|\n| Tom Brand|\n+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession,Row\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndata=[(\"James\",23),(\"Ann\",40)]\ndf=spark.createDataFrame(data).toDF(\"name.fname\",\"gender\")\ndf.printSchema()\ndf.show()\n\nfrom pyspark.sql.functions import col\ndf.select(col(\"`name.fname`\")).show()\ndf.select(df[\"`name.fname`\"]).show()\ndf.withColumn(\"new_col\",col(\"`name.fname`\").substr(1,2)).show()\ndf.filter(col(\"`name.fname`\").startswith(\"J\")).show()\nnew_cols=(column.replace('.', '_') for column in df.columns)\ndf2 = df.toDF(*new_cols)\ndf2.show()\n\n\n# Using DataFrame object\ndf.select(df.gender).show()\ndf.select(df[\"gender\"]).show()\n#Accessing column name with dot (with backticks)\ndf.select(df[\"`name.fname`\"]).show()\n\n#Using SQL col() function\nfrom pyspark.sql.functions import col\ndf.select(col(\"gender\")).show()\n#Accessing column name with dot (with backticks)\ndf.select(col(\"`name.fname`\")).show()\n\n#Access struct column\ndata=[Row(name=\"James\",prop=Row(hair=\"black\",eye=\"blue\")),\n      Row(name=\"Ann\",prop=Row(hair=\"grey\",eye=\"black\"))]\ndf=spark.createDataFrame(data)\ndf.printSchema()\n\ndf.select(df.prop.hair).show()\ndf.select(df[\"prop.hair\"]).show()\ndf.select(col(\"prop.hair\")).show()\ndf.select(col(\"prop.*\")).show()\n\n# Column operators\ndata=[(100,2,1),(200,3,4),(300,4,4)]\ndf=spark.createDataFrame(data).toDF(\"col1\",\"col2\",\"col3\")\ndf.select(df.col1 + df.col2).show()\ndf.select(df.col1 - df.col2).show() \ndf.select(df.col1 * df.col2).show()\ndf.select(df.col1 / df.col2).show()\ndf.select(df.col1 % df.col2).show()\n\ndf.select(df.col2 > df.col3).show()\ndf.select(df.col2 < df.col3).show()\ndf.select(df.col2 == df.col3).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"3bee6072-484a-484c-b953-f0e1451402b7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name.fname: string (nullable = true)\n |-- gender: long (nullable = true)\n\n+----------+------+\n|name.fname|gender|\n+----------+------+\n|     James|    23|\n|       Ann|    40|\n+----------+------+\n\n+----------+\n|name.fname|\n+----------+\n|     James|\n|       Ann|\n+----------+\n\n+----------+\n|name.fname|\n+----------+\n|     James|\n|       Ann|\n+----------+\n\n+----------+------+-------+\n|name.fname|gender|new_col|\n+----------+------+-------+\n|     James|    23|     Ja|\n|       Ann|    40|     An|\n+----------+------+-------+\n\n+----------+------+\n|name.fname|gender|\n+----------+------+\n|     James|    23|\n+----------+------+\n\n+----------+------+\n|name_fname|gender|\n+----------+------+\n|     James|    23|\n|       Ann|    40|\n+----------+------+\n\n+------+\n|gender|\n+------+\n|    23|\n|    40|\n+------+\n\n+------+\n|gender|\n+------+\n|    23|\n|    40|\n+------+\n\n+----------+\n|name.fname|\n+----------+\n|     James|\n|       Ann|\n+----------+\n\n+------+\n|gender|\n+------+\n|    23|\n|    40|\n+------+\n\n+----------+\n|name.fname|\n+----------+\n|     James|\n|       Ann|\n+----------+\n\nroot\n |-- name: string (nullable = true)\n |-- prop: struct (nullable = true)\n |    |-- hair: string (nullable = true)\n |    |-- eye: string (nullable = true)\n\n+---------+\n|prop.hair|\n+---------+\n|    black|\n|     grey|\n+---------+\n\n+-----+\n| hair|\n+-----+\n|black|\n| grey|\n+-----+\n\n+-----+\n| hair|\n+-----+\n|black|\n| grey|\n+-----+\n\n+-----+-----+\n| hair|  eye|\n+-----+-----+\n|black| blue|\n| grey|black|\n+-----+-----+\n\n+-------------+\n|(col1 + col2)|\n+-------------+\n|          102|\n|          203|\n|          304|\n+-------------+\n\n+-------------+\n|(col1 - col2)|\n+-------------+\n|           98|\n|          197|\n|          296|\n+-------------+\n\n+-------------+\n|(col1 * col2)|\n+-------------+\n|          200|\n|          600|\n|         1200|\n+-------------+\n\n+-----------------+\n|    (col1 / col2)|\n+-----------------+\n|             50.0|\n|66.66666666666667|\n|             75.0|\n+-----------------+\n\n+-------------+\n|(col1 % col2)|\n+-------------+\n|            0|\n|            2|\n|            0|\n+-------------+\n\n+-------------+\n|(col2 > col3)|\n+-------------+\n|         true|\n|        false|\n|        false|\n+-------------+\n\n+-------------+\n|(col2 < col3)|\n+-------------+\n|        false|\n|         true|\n|        false|\n+-------------+\n\n+-------------+\n|(col2 = col3)|\n+-------------+\n|        false|\n|        false|\n|         true|\n+-------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndataDictionary = [\n        ('James',{'hair':'black','eye':'brown'}),\n        ('Michael',{'hair':'brown','eye':None}),\n        ('Robert',{'hair':'red','eye':'black'}),\n        ('Washington',{'hair':'grey','eye':'grey'}),\n        ('Jefferson',{'hair':'brown','eye':''})\n        ]\n\ndf = spark.createDataFrame(data=dataDictionary, schema = ['name','properties'])\ndf.printSchema()\ndf.show(truncate=False)\n\ndf3=df.rdd.map(lambda x: \\\n    (x.name,x.properties[\"hair\"],x.properties[\"eye\"])) \\\n    .toDF([\"name\",\"hair\",\"eye\"])\ndf3.printSchema()\ndf3.show()\n\ndf.withColumn(\"hair\",df.properties.getItem(\"hair\")) \\\n  .withColumn(\"eye\",df.properties.getItem(\"eye\")) \\\n  .drop(\"properties\") \\\n  .show()\n\ndf.withColumn(\"hair\",df.properties[\"hair\"]) \\\n  .withColumn(\"eye\",df.properties[\"eye\"]) \\\n  .drop(\"properties\") \\\n  .show()\n\n# Functions\nfrom pyspark.sql.functions import explode,map_keys,col\nkeysDF = df.select(explode(map_keys(df.properties))).distinct()\nkeysList = keysDF.rdd.map(lambda x:x[0]).collect()\nkeyCols = list(map(lambda x: col(\"properties\").getItem(x).alias(str(x)), keysList))\ndf.select(df.name, *keyCols).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0bb3e240-5fdd-4591-9338-33e1f2f5d428","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+----------+-----------------------------+\n|name      |properties                   |\n+----------+-----------------------------+\n|James     |{eye -> brown, hair -> black}|\n|Michael   |{eye -> null, hair -> brown} |\n|Robert    |{eye -> black, hair -> red}  |\n|Washington|{eye -> grey, hair -> grey}  |\n|Jefferson |{eye -> , hair -> brown}     |\n+----------+-----------------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- hair: string (nullable = true)\n |-- eye: string (nullable = true)\n\n+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n+----------+-----+-----+\n|      name|  eye| hair|\n+----------+-----+-----+\n|     James|brown|black|\n|   Michael| null|brown|\n|    Robert|black|  red|\n|Washington| grey| grey|\n| Jefferson|     |brown|\n+----------+-----+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndata = [ (\"36636\",\"Finance\",3000,\"USA\"), \n    (\"40288\",\"Finance\",5000,\"IND\"), \n    (\"42114\",\"Sales\",3900,\"USA\"), \n    (\"39192\",\"Marketing\",2500,\"CAN\"), \n    (\"34534\",\"Sales\",6500,\"USA\") ]\nschema = StructType([\n     StructField('id', StringType(), True),\n     StructField('dept', StringType(), True),\n     StructField('salary', IntegerType(), True),\n     StructField('location', StringType(), True)\n     ])\n\ndf = spark.createDataFrame(data=data,schema=schema)\ndf.printSchema()\ndf.show(truncate=False)\n\n#Convert scolumns to Map\nfrom pyspark.sql.functions import col,lit,create_map\ndf = df.withColumn(\"propertiesMap\",create_map(\n        lit(\"salary\"),col(\"salary\"),\n        lit(\"location\"),col(\"location\")\n        )).drop(\"salary\",\"location\")\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4fbce185-8c2b-4aa1-9be8-1f8059b49790","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- id: string (nullable = true)\n |-- dept: string (nullable = true)\n |-- salary: integer (nullable = true)\n |-- location: string (nullable = true)\n\n+-----+---------+------+--------+\n|id   |dept     |salary|location|\n+-----+---------+------+--------+\n|36636|Finance  |3000  |USA     |\n|40288|Finance  |5000  |IND     |\n|42114|Sales    |3900  |USA     |\n|39192|Marketing|2500  |CAN     |\n|34534|Sales    |6500  |USA     |\n+-----+---------+------+--------+\n\nroot\n |-- id: string (nullable = true)\n |-- dept: string (nullable = true)\n |-- propertiesMap: map (nullable = false)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+-----+---------+---------------------------------+\n|id   |dept     |propertiesMap                    |\n+-----+---------+---------------------------------+\n|36636|Finance  |{salary -> 3000, location -> USA}|\n|40288|Finance  |{salary -> 5000, location -> IND}|\n|42114|Sales    |{salary -> 3900, location -> USA}|\n|39192|Marketing|{salary -> 2500, location -> CAN}|\n|34534|Sales    |{salary -> 6500, location -> USA}|\n+-----+---------+---------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n         .appName('SparkByExamples.com') \\\n         .getOrCreate()\n\ndata = [(\"James\", \"Sales\", 3000),\n    (\"Michael\", \"Sales\", 4600),\n    (\"Robert\", \"Sales\", 4100),\n    (\"Maria\", \"Finance\", 3000),\n    (\"James\", \"Sales\", 3000),\n    (\"Scott\", \"Finance\", 3300),\n    (\"Jen\", \"Finance\", 3900),\n    (\"Jeff\", \"Marketing\", 3000),\n    (\"Kumar\", \"Marketing\", 2000),\n    (\"Saif\", \"Sales\", 4100)\n  ]\ncolumns = [\"Name\",\"Dept\",\"Salary\"]\ndf = spark.createDataFrame(data=data,schema=columns)\ndf.distinct().show()\nprint(\"Distinct Count: \" + str(df.distinct().count()))\n\n# Using countDistrinct()\nfrom pyspark.sql.functions import countDistinct\ndf2=df.select(countDistinct(\"Dept\",\"Salary\"))\ndf2.show()\n\nprint(\"Distinct Count of Department &amp; Salary: \"+ str(df2.collect()[0][0]))\n\ndf.createOrReplaceTempView(\"PERSON\")\nspark.sql(\"select distinct(count(*)) from PERSON\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"9900eac5-8272-445d-9590-31cd97d4e545","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+---------+------+\n|   Name|     Dept|Salary|\n+-------+---------+------+\n|  James|    Sales|  3000|\n|Michael|    Sales|  4600|\n| Robert|    Sales|  4100|\n|  Maria|  Finance|  3000|\n|  Scott|  Finance|  3300|\n|    Jen|  Finance|  3900|\n|   Jeff|Marketing|  3000|\n|  Kumar|Marketing|  2000|\n|   Saif|    Sales|  4100|\n+-------+---------+------+\n\nDistinct Count: 9\n+----------------------------+\n|count(DISTINCT Dept, Salary)|\n+----------------------------+\n|                           8|\n+----------------------------+\n\nDistinct Count of Department &amp; Salary: 8\n+--------+\n|count(1)|\n+--------+\n|      10|\n+--------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndataDictionary = [\n        ('James',{'hair':'black','eye':'brown'}),\n        ('Michael',{'hair':'brown','eye':None}),\n        ('Robert',{'hair':'red','eye':'black'}),\n        ('Washington',{'hair':'grey','eye':'grey'}),\n        ('Jefferson',{'hair':'brown','eye':''})\n        ]\n\ndf = spark.createDataFrame(data=dataDictionary, schema = ['name','properties'])\ndf.printSchema()\ndf.show(truncate=False)\n\n# Using StructType schema\nfrom pyspark.sql.types import StructField, StructType, StringType, MapType,IntegerType\nschema = StructType([\n    StructField('name', StringType(), True),\n    StructField('properties', MapType(StringType(),StringType()),True)\n])\ndf2 = spark.createDataFrame(data=dataDictionary, schema = schema)\ndf2.printSchema()\ndf2.show(truncate=False)\n\ndf3=df.rdd.map(lambda x: \\\n    (x.name,x.properties[\"hair\"],x.properties[\"eye\"])) \\\n    .toDF([\"name\",\"hair\",\"eye\"])\ndf3.printSchema()\ndf3.show()\n\ndf.withColumn(\"hair\",df.properties.getItem(\"hair\")) \\\n  .withColumn(\"eye\",df.properties.getItem(\"eye\")) \\\n  .drop(\"properties\") \\\n  .show()\n\ndf.withColumn(\"hair\",df.properties[\"hair\"]) \\\n  .withColumn(\"eye\",df.properties[\"eye\"]) \\\n  .drop(\"properties\") \\\n  .show()\n\n# Functions\nfrom pyspark.sql.functions import explode,map_keys,col\nkeysDF = df.select(explode(map_keys(df.properties))).distinct()\nkeysList = keysDF.rdd.map(lambda x:x[0]).collect()\nkeyCols = list(map(lambda x: col(\"properties\").getItem(x).alias(str(x)), keysList))\ndf.select(df.name, *keyCols).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"037b1016-aae6-4352-9347-b98380a57a3b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+----------+-----------------------------+\n|name      |properties                   |\n+----------+-----------------------------+\n|James     |{eye -> brown, hair -> black}|\n|Michael   |{eye -> null, hair -> brown} |\n|Robert    |{eye -> black, hair -> red}  |\n|Washington|{eye -> grey, hair -> grey}  |\n|Jefferson |{eye -> , hair -> brown}     |\n+----------+-----------------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+----------+-----------------------------+\n|name      |properties                   |\n+----------+-----------------------------+\n|James     |{eye -> brown, hair -> black}|\n|Michael   |{eye -> null, hair -> brown} |\n|Robert    |{eye -> black, hair -> red}  |\n|Washington|{eye -> grey, hair -> grey}  |\n|Jefferson |{eye -> , hair -> brown}     |\n+----------+-----------------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- hair: string (nullable = true)\n |-- eye: string (nullable = true)\n\n+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n+----------+-----+-----+\n|      name|  eye| hair|\n+----------+-----+-----+\n|     James|brown|black|\n|   Michael| null|brown|\n|    Robert|black|  red|\n|Washington| grey| grey|\n| Jefferson|     |brown|\n+----------+-----+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession, Row\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\nfrom pyspark.sql.functions import *\n\ncolumns = [\"language\",\"users_count\"]\ndata = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\nrdd = spark.sparkContext.parallelize(data)\n\ndfFromRDD1 = rdd.toDF()\ndfFromRDD1.printSchema()\n\ndfFromRDD1 = rdd.toDF(columns)\ndfFromRDD1.printSchema()\n\ndfFromRDD2 = spark.createDataFrame(rdd).toDF(*columns)\ndfFromRDD2.printSchema()\n\ndfFromData2 = spark.createDataFrame(data).toDF(*columns)\ndfFromData2.printSchema()     \n\nrowData = map(lambda x: Row(*x), data) \ndfFromData3 = spark.createDataFrame(rowData,columns)\ndfFromData3.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a44f88d7-f47d-4950-a986-2651304392b0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- _1: string (nullable = true)\n |-- _2: string (nullable = true)\n\nroot\n |-- language: string (nullable = true)\n |-- users_count: string (nullable = true)\n\nroot\n |-- language: string (nullable = true)\n |-- users_count: string (nullable = true)\n\nroot\n |-- language: string (nullable = true)\n |-- users_count: string (nullable = true)\n\nroot\n |-- language: string (nullable = true)\n |-- users_count: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession, Row\nfrom pyspark.sql.types import StructType,StructField, StringType\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\n#Using List\ndept = [(\"Finance\",10), \n        (\"Marketing\",20), \n        (\"Sales\",30), \n        (\"IT\",40) \n      ]\n\ndeptColumns = [\"dept_name\",\"dept_id\"]\ndeptDF = spark.createDataFrame(data=dept, schema = deptColumns)\ndeptDF.printSchema()\ndeptDF.show(truncate=False)\n\ndeptSchema = StructType([       \n    StructField('dept_name', StringType(), True),\n    StructField('dept_id', StringType(), True)\n])\n\ndeptDF1 = spark.createDataFrame(data=dept, schema = deptSchema)\ndeptDF1.printSchema()\ndeptDF1.show(truncate=False)\n\n# Using list of Row type\ndept2 = [Row(\"Finance\",10), \n        Row(\"Marketing\",20), \n        Row(\"Sales\",30), \n        Row(\"IT\",40) \n      ]\n\ndeptDF2 = spark.createDataFrame(data=dept2, schema = deptColumns)\ndeptDF2.printSchema()\ndeptDF2.show(truncate=False)\n\n# Convert list to RDD\nrdd = spark.sparkContext.parallelize(dept)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"7ed38b20-f2ce-4d56-a9e8-20a9b0f00e4e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\nroot\n |-- dept_name: string (nullable = true)\n |-- dept_id: string (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\nroot\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\n# Create SparkSession\nspark = SparkSession.builder \\\n               .appName('SparkByExamples.com') \\\n               .getOrCreate()\ndata=[[\"1\"]]\ndf=spark.createDataFrame(data,[\"id\"])\n\nfrom pyspark.sql.functions import *\n\n#current_date() & current_timestamp()\ndf.withColumn(\"current_date\",current_date()) \\\n  .withColumn(\"current_timestamp\",current_timestamp()) \\\n  .show(truncate=False)\n\n#SQL\nspark.sql(\"select current_date(), current_timestamp()\") \\\n     .show(truncate=False)\n\n# Date & Timestamp into custom format\ndf.withColumn(\"date_format\",date_format(current_date(),\"MM-dd-yyyy\")) \\\n  .withColumn(\"to_timestamp\",to_timestamp(current_timestamp(),\"MM-dd-yyyy HH mm ss SSS\")) \\\n  .show(truncate=False)\n\n#SQL\nspark.sql(\"select date_format(current_date(),'MM-dd-yyyy') as date_format ,\" + \\\n          \"to_timestamp(current_timestamp(),'MM-dd-yyyy HH mm ss SSS') as to_timestamp\") \\\n     .show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"cba0ead8-f125-4495-9e9f-8fe61418e8a5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+------------+----------------------+\n|id |current_date|current_timestamp     |\n+---+------------+----------------------+\n|1  |2023-06-18  |2023-06-18 14:59:20.88|\n+---+------------+----------------------+\n\n+--------------+----------------------+\n|current_date()|current_timestamp()   |\n+--------------+----------------------+\n|2023-06-18    |2023-06-18 14:59:21.28|\n+--------------+----------------------+\n\n+---+-----------+-----------------------+\n|id |date_format|to_timestamp           |\n+---+-----------+-----------------------+\n|1  |06-18-2023 |2023-06-18 14:59:21.439|\n+---+-----------+-----------------------+\n\n+-----------+----------------------+\n|date_format|to_timestamp          |\n+-----------+----------------------+\n|06-18-2023 |2023-06-18 14:59:21.84|\n+-----------+----------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ncolumns = [\"name\",\"languagesAtSchool\",\"currentState\"]\ndata = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n\ndf = spark.createDataFrame(data=data,schema=columns)\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"9795b710-87f6-4d61-a524-fd12f5df6c10","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- languagesAtSchool: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- currentState: string (nullable = true)\n\n+----------------+------------------+------------+\n|name            |languagesAtSchool |currentState|\n+----------------+------------------+------------+\n|James,,Smith    |[Java, Scala, C++]|CA          |\n|Michael,Rose,   |[Spark, Java, C++]|NJ          |\n|Robert,,Williams|[CSharp, VB]      |NV          |\n+----------------+------------------+------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com') \\\n        .master(\"local[5]\").getOrCreate()\n\ndf=spark.range(0,20)\nprint(df.rdd.getNumPartitions())\n\ndf.write.mode(\"overwrite\").csv(\"partition.csv\")\n\ndf2 = df.repartition(6)\nprint(df2.rdd.getNumPartitions())\n\ndf3 = df.coalesce(2)\nprint(df3.rdd.getNumPartitions())\n\ndf4 = df.groupBy(\"id\").count()\nprint(df4.rdd.getNumPartitions())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c387ca8f-1caa-4fab-8896-e82d6a9f8560","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["8\n6\n2\n8\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, lit\nfrom pyspark.sql.types import StructType, StructField, StringType,IntegerType\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nprint(spark)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"620f8838-de77-4e5e-8803-ea69a0114d0e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["<pyspark.sql.session.SparkSession object at 0x7f607c43c400>\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\n# Create SparkSession\nspark = SparkSession.builder \\\n               .appName('SparkByExamples.com') \\\n               .getOrCreate()\n\nfrom pyspark.sql.functions import *\n\ndf=spark.createDataFrame([[\"1\"]],[\"id\"])\ndf.select(current_date().alias(\"current_date\"), \\\n      date_format(current_date(),\"yyyy MM dd\").alias(\"yyyy MM dd\"), \\\n      date_format(current_timestamp(),\"MM/dd/yyyy hh:mm\").alias(\"MM/dd/yyyy\"), \\\n      date_format(current_timestamp(),\"yyyy MMM dd\").alias(\"yyyy MMMM dd\"), \\\n      date_format(current_timestamp(),\"yyyy MMMM dd E\").alias(\"yyyy MMMM dd E\") \\\n   ).show()\n\n#SQL\n\nspark.sql(\"select current_date() as current_date, \"+\n      \"date_format(current_timestamp(),'yyyy MM dd') as yyyy_MM_dd, \"+\n      \"date_format(current_timestamp(),'MM/dd/yyyy hh:mm') as MM_dd_yyyy, \"+\n      \"date_format(current_timestamp(),'yyyy MMM dd') as yyyy_MMMM_dd, \"+\n      \"date_format(current_timestamp(),'yyyy MMMM dd E') as yyyy_MMMM_dd_E\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e0a2ef67-ebb7-4c0d-b6a1-91baedefa802","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------------+----------+----------------+------------+----------------+\n|current_date|yyyy MM dd|      MM/dd/yyyy|yyyy MMMM dd|  yyyy MMMM dd E|\n+------------+----------+----------------+------------+----------------+\n|  2023-06-18|2023 06 18|06/18/2023 02:59| 2023 Jun 18|2023 June 18 Sun|\n+------------+----------+----------------+------------+----------------+\n\n+------------+----------+----------------+------------+----------------+\n|current_date|yyyy_MM_dd|      MM_dd_yyyy|yyyy_MMMM_dd|  yyyy_MMMM_dd_E|\n+------------+----------+----------------+------------+----------------+\n|  2023-06-18|2023 06 18|06/18/2023 02:59| 2023 Jun 18|2023 June 18 Sun|\n+------------+----------+----------------+------------+----------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n# Create SparkSession\nspark = SparkSession.builder \\\n               .appName('SparkByExamples.com') \\\n               .getOrCreate()\ndata=[[\"1\",\"2020-02-01\"],[\"2\",\"2019-03-01\"],[\"3\",\"2021-03-01\"]]\ndf=spark.createDataFrame(data,[\"id\",\"input\"])\ndf.show()\n\nfrom pyspark.sql.functions import *\n\n#current_date()\ndf.select(current_date().alias(\"current_date\")\n  ).show(1)\n\n#date_format()\ndf.select(col(\"input\"), \n    date_format(col(\"input\"), \"MM-dd-yyyy\").alias(\"date_format\") \n  ).show()\n\n#to_date()\ndf.select(col(\"input\"), \n    to_date(col(\"input\"), \"yyy-MM-dd\").alias(\"to_date\") \n  ).show()\n\n#datediff()\ndf.select(col(\"input\"), \n    datediff(current_date(),col(\"input\")).alias(\"datediff\")  \n  ).show()\n\n#months_between()\ndf.select(col(\"input\"), \n    months_between(current_date(),col(\"input\")).alias(\"months_between\")  \n  ).show()\n\n#trunc()\ndf.select(col(\"input\"), \n    trunc(col(\"input\"),\"Month\").alias(\"Month_Trunc\"), \n    trunc(col(\"input\"),\"Year\").alias(\"Month_Year\"), \n    trunc(col(\"input\"),\"Month\").alias(\"Month_Trunc\")\n   ).show()\n\n#add_months() , date_add(), date_sub()\n\ndf.select(col(\"input\"), \n    add_months(col(\"input\"),3).alias(\"add_months\"), \n    add_months(col(\"input\"),-3).alias(\"sub_months\"), \n    date_add(col(\"input\"),4).alias(\"date_add\"), \n    date_sub(col(\"input\"),4).alias(\"date_sub\") \n  ).show()\n\n#\n\ndf.select(col(\"input\"), \n     year(col(\"input\")).alias(\"year\"), \n     month(col(\"input\")).alias(\"month\"), \n     next_day(col(\"input\"),\"Sunday\").alias(\"next_day\"), \n     weekofyear(col(\"input\")).alias(\"weekofyear\") \n  ).show()\n\ndf.select(col(\"input\"),  \n     dayofweek(col(\"input\")).alias(\"dayofweek\"), \n     dayofmonth(col(\"input\")).alias(\"dayofmonth\"), \n     dayofyear(col(\"input\")).alias(\"dayofyear\"), \n  ).show()\n\ndata=[[\"1\",\"02-01-2020 11 01 19 06\"],[\"2\",\"03-01-2019 12 01 19 406\"],[\"3\",\"03-01-2021 12 01 19 406\"]]\ndf2=spark.createDataFrame(data,[\"id\",\"input\"])\ndf2.show(truncate=False)\n\n#current_timestamp()\ndf2.select(current_timestamp().alias(\"current_timestamp\")\n  ).show(1,truncate=False)\n\n#to_timestamp()\ndf2.select(col(\"input\"), \n    to_timestamp(col(\"input\"), \"MM-dd-yyyy HH mm ss SSS\").alias(\"to_timestamp\") \n  ).show(truncate=False)\n\n\n#hour, minute,second\ndata=[[\"1\",\"2020-02-01 11:01:19.06\"],[\"2\",\"2019-03-01 12:01:19.406\"],[\"3\",\"2021-03-01 12:01:19.406\"]]\ndf3=spark.createDataFrame(data,[\"id\",\"input\"])\n\ndf3.select(col(\"input\"), \n    hour(col(\"input\")).alias(\"hour\"), \n    minute(col(\"input\")).alias(\"minute\"),\n    second(col(\"input\")).alias(\"second\") \n  ).show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d97f3edd-d28c-4d78-8eb0-4b9ff10a4d11","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----------+\n| id|     input|\n+---+----------+\n|  1|2020-02-01|\n|  2|2019-03-01|\n|  3|2021-03-01|\n+---+----------+\n\n+------------+\n|current_date|\n+------------+\n|  2023-06-18|\n+------------+\nonly showing top 1 row\n\n+----------+-----------+\n|     input|date_format|\n+----------+-----------+\n|2020-02-01| 02-01-2020|\n|2019-03-01| 03-01-2019|\n|2021-03-01| 03-01-2021|\n+----------+-----------+\n\n+----------+----------+\n|     input|   to_date|\n+----------+----------+\n|2020-02-01|2020-02-01|\n|2019-03-01|2019-03-01|\n|2021-03-01|2021-03-01|\n+----------+----------+\n\n+----------+--------+\n|     input|datediff|\n+----------+--------+\n|2020-02-01|    1233|\n|2019-03-01|    1570|\n|2021-03-01|     839|\n+----------+--------+\n\n+----------+--------------+\n|     input|months_between|\n+----------+--------------+\n|2020-02-01|    40.5483871|\n|2019-03-01|    51.5483871|\n|2021-03-01|    27.5483871|\n+----------+--------------+\n\n+----------+-----------+----------+-----------+\n|     input|Month_Trunc|Month_Year|Month_Trunc|\n+----------+-----------+----------+-----------+\n|2020-02-01| 2020-02-01|2020-01-01| 2020-02-01|\n|2019-03-01| 2019-03-01|2019-01-01| 2019-03-01|\n|2021-03-01| 2021-03-01|2021-01-01| 2021-03-01|\n+----------+-----------+----------+-----------+\n\n+----------+----------+----------+----------+----------+\n|     input|add_months|sub_months|  date_add|  date_sub|\n+----------+----------+----------+----------+----------+\n|2020-02-01|2020-05-01|2019-11-01|2020-02-05|2020-01-28|\n|2019-03-01|2019-06-01|2018-12-01|2019-03-05|2019-02-25|\n|2021-03-01|2021-06-01|2020-12-01|2021-03-05|2021-02-25|\n+----------+----------+----------+----------+----------+\n\n+----------+----+-----+----------+----------+\n|     input|year|month|  next_day|weekofyear|\n+----------+----+-----+----------+----------+\n|2020-02-01|2020|    2|2020-02-02|         5|\n|2019-03-01|2019|    3|2019-03-03|         9|\n|2021-03-01|2021|    3|2021-03-07|         9|\n+----------+----+-----+----------+----------+\n\n+----------+---------+----------+---------+\n|     input|dayofweek|dayofmonth|dayofyear|\n+----------+---------+----------+---------+\n|2020-02-01|        7|         1|       32|\n|2019-03-01|        6|         1|       60|\n|2021-03-01|        2|         1|       60|\n+----------+---------+----------+---------+\n\n+---+-----------------------+\n|id |input                  |\n+---+-----------------------+\n|1  |02-01-2020 11 01 19 06 |\n|2  |03-01-2019 12 01 19 406|\n|3  |03-01-2021 12 01 19 406|\n+---+-----------------------+\n\n+-----------------------+\n|current_timestamp      |\n+-----------------------+\n|2023-06-18 14:59:39.536|\n+-----------------------+\nonly showing top 1 row\n\n+-----------------------+-----------------------+\n|input                  |to_timestamp           |\n+-----------------------+-----------------------+\n|02-01-2020 11 01 19 06 |2020-02-01 11:01:19.06 |\n|03-01-2019 12 01 19 406|2019-03-01 12:01:19.406|\n|03-01-2021 12 01 19 406|2021-03-01 12:01:19.406|\n+-----------------------+-----------------------+\n\n+-----------------------+----+------+------+\n|input                  |hour|minute|second|\n+-----------------------+----+------+------+\n|2020-02-01 11:01:19.06 |11  |1     |19    |\n|2019-03-01 12:01:19.406|12  |1     |19    |\n|2021-03-01 12:01:19.406|12  |1     |19    |\n+-----------------------+----+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\n# Create SparkSession\nspark = SparkSession.builder \\\n          .appName('SparkByExamples.com') \\\n          .getOrCreate()\ndata = [(\"1\",\"2019-07-01\"),(\"2\",\"2019-06-24\"),(\"3\",\"2019-08-24\")]\n\ndf=spark.createDataFrame(data=data,schema=[\"id\",\"date\"])\n\nfrom pyspark.sql.functions import *\n\ndf.select(\n      col(\"date\"),\n      current_date().alias(\"current_date\"),\n      datediff(current_date(),col(\"date\")).alias(\"datediff\")\n    ).show()\n\ndf.withColumn(\"datesDiff\", datediff(current_date(),col(\"date\"))) \\\n  .withColumn(\"montsDiff\", months_between(current_date(),col(\"date\"))) \\\n  .withColumn(\"montsDiff_round\",round(months_between(current_date(),col(\"date\")),2)) \\\n  .withColumn(\"yearsDiff\",months_between(current_date(),col(\"date\"))/lit(12)) \\\n  .withColumn(\"yearsDiff_round\",round(months_between(current_date(),col(\"date\"))/lit(12),2)) \\\n  .show()\n\ndata2 = [(\"1\",\"07-01-2019\"),(\"2\",\"06-24-2019\"),(\"3\",\"08-24-2019\")]  \ndf2=spark.createDataFrame(data=data2,schema=[\"id\",\"date\"])\ndf2.select(\n    to_date(col(\"date\"),\"MM-dd-yyyy\").alias(\"date\"),\n    current_date().alias(\"endDate\")\n    )\n\n#SQL\n\nspark.sql(\"select round(months_between('2019-07-01',current_date())/12,2) as years_diff\").show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e9b595ec-884b-4053-afed-73d58fc65b6c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+------------+--------+\n|      date|current_date|datediff|\n+----------+------------+--------+\n|2019-07-01|  2023-06-18|    1448|\n|2019-06-24|  2023-06-18|    1455|\n|2019-08-24|  2023-06-18|    1394|\n+----------+------------+--------+\n\n+---+----------+---------+-----------+---------------+------------------+---------------+\n| id|      date|datesDiff|  montsDiff|montsDiff_round|         yearsDiff|yearsDiff_round|\n+---+----------+---------+-----------+---------------+------------------+---------------+\n|  1|2019-07-01|     1448| 47.5483871|          47.55|3.9623655916666665|           3.96|\n|  2|2019-06-24|     1455|47.80645161|          47.81|      3.9838709675|           3.98|\n|  3|2019-08-24|     1394|45.80645161|          45.81|3.8172043008333336|           3.82|\n+---+----------+---------+-----------+---------------+------------------+---------------+\n\n+----------+\n|years_diff|\n+----------+\n|     -3.96|\n+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import expr\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndata = [(\"James\", \"Sales\", 3000), \\\n    (\"Michael\", \"Sales\", 4600), \\\n    (\"Robert\", \"Sales\", 4100), \\\n    (\"Maria\", \"Finance\", 3000), \\\n    (\"James\", \"Sales\", 3000), \\\n    (\"Scott\", \"Finance\", 3300), \\\n    (\"Jen\", \"Finance\", 3900), \\\n    (\"Jeff\", \"Marketing\", 3000), \\\n    (\"Kumar\", \"Marketing\", 2000), \\\n    (\"Saif\", \"Sales\", 4100) \\\n  ]\ncolumns= [\"employee_name\", \"department\", \"salary\"]\ndf = spark.createDataFrame(data = data, schema = columns)\ndf.printSchema()\ndf.show(truncate=False)\n\ndistinctDF = df.distinct()\nprint(\"Distinct count: \"+str(distinctDF.count()))\ndistinctDF.show(truncate=False)\n\ndf2 = df.dropDuplicates()\nprint(\"Distinct count: \"+str(df2.count()))\ndf2.show(truncate=False)\n\ndropDisDF = df.dropDuplicates([\"department\",\"salary\"])\nprint(\"Distinct count of department salary : \"+str(dropDisDF.count()))\ndropDisDF.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c7130a88-9437-4ce2-8c83-9607e54f9056","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\nDistinct count: 9\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\nDistinct count: 9\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\nDistinct count of department salary : 8\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|Maria        |Finance   |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Kumar        |Marketing |2000  |\n|Jeff         |Marketing |3000  |\n|James        |Sales     |3000  |\n|Robert       |Sales     |4100  |\n|Michael      |Sales     |4600  |\n+-------------+----------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\nsimpleData = ((\"James\",\"\",\"Smith\",\"36636\",\"NewYork\",3100), \\\n    (\"Michael\",\"Rose\",\"\",\"40288\",\"California\",4300), \\\n    (\"Robert\",\"\",\"Williams\",\"42114\",\"Florida\",1400), \\\n    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"Florida\",5500), \\\n    (\"Jen\",\"Mary\",\"Brown\",\"34561\",\"NewYork\",3000) \\\n  )\ncolumns= [\"firstname\",\"middlename\",\"lastname\",\"id\",\"location\",\"salary\"]\n\ndf = spark.createDataFrame(data = simpleData, schema = columns)\n\ndf.printSchema()\ndf.show(truncate=False)\n\ndf.drop(\"firstname\") \\\n  .printSchema()\n  \ndf.drop(col(\"firstname\")) \\\n  .printSchema()  \n  \ndf.drop(df.firstname) \\\n  .printSchema()\n\ndf.drop(\"firstname\",\"middlename\",\"lastname\") \\\n    .printSchema()\n\ncols = (\"firstname\",\"middlename\",\"lastname\")\n\ndf.drop(*cols) \\\n   .printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"2d1787e7-a57c-48cb-8d7b-391cec9929a2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- location: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+---------+----------+--------+-----+----------+------+\n|firstname|middlename|lastname|id   |location  |salary|\n+---------+----------+--------+-----+----------+------+\n|James    |          |Smith   |36636|NewYork   |3100  |\n|Michael  |Rose      |        |40288|California|4300  |\n|Robert   |          |Williams|42114|Florida   |1400  |\n|Maria    |Anne      |Jones   |39192|Florida   |5500  |\n|Jen      |Mary      |Brown   |34561|NewYork   |3000  |\n+---------+----------+--------+-----+----------+------+\n\nroot\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- location: string (nullable = true)\n |-- salary: long (nullable = true)\n\nroot\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- location: string (nullable = true)\n |-- salary: long (nullable = true)\n\nroot\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- location: string (nullable = true)\n |-- salary: long (nullable = true)\n\nroot\n |-- id: string (nullable = true)\n |-- location: string (nullable = true)\n |-- salary: long (nullable = true)\n\nroot\n |-- id: string (nullable = true)\n |-- location: string (nullable = true)\n |-- salary: long (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\nsimpleData = ((\"James\",\"\",\"Smith\",\"36636\",\"NewYork\",3100), \\\n    (\"Michael\",\"Rose\",\"\",\"40288\",\"California\",4300), \\\n    (\"Robert\",\"\",\"Williams\",\"42114\",\"Florida\",1400), \\\n    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"Florida\",5500), \\\n    (\"Jen\",\"Mary\",\"Brown\",\"34561\",\"NewYork\",3000) \\\n  )\ncolumns= [\"firstname\",\"middlename\",\"lastname\",\"id\",\"location\",\"salary\"]\n\ndf = spark.createDataFrame(data = simpleData, schema = columns)\n\ndf.printSchema()\ndf.show(truncate=False)\n\ndf.drop(\"firstname\") \\\n  .printSchema()\n  \ndf.drop(col(\"firstname\")) \\\n  .printSchema()  \n  \ndf.drop(df.firstname) \\\n  .printSchema()\n\ndf.drop(\"firstname\",\"middlename\",\"lastname\") \\\n    .printSchema()\n\ncols = (\"firstname\",\"middlename\",\"lastname\")\n\ndf.drop(*cols) \\\n   .printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"79001df3-663e-45d6-ba34-45170430cf72","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- location: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+---------+----------+--------+-----+----------+------+\n|firstname|middlename|lastname|id   |location  |salary|\n+---------+----------+--------+-----+----------+------+\n|James    |          |Smith   |36636|NewYork   |3100  |\n|Michael  |Rose      |        |40288|California|4300  |\n|Robert   |          |Williams|42114|Florida   |1400  |\n|Maria    |Anne      |Jones   |39192|Florida   |5500  |\n|Jen      |Mary      |Brown   |34561|NewYork   |3000  |\n+---------+----------+--------+-----+----------+------+\n\nroot\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- location: string (nullable = true)\n |-- salary: long (nullable = true)\n\nroot\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- location: string (nullable = true)\n |-- salary: long (nullable = true)\n\nroot\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- location: string (nullable = true)\n |-- salary: long (nullable = true)\n\nroot\n |-- id: string (nullable = true)\n |-- location: string (nullable = true)\n |-- salary: long (nullable = true)\n\nroot\n |-- id: string (nullable = true)\n |-- location: string (nullable = true)\n |-- salary: long (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField, StringType\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nschema = StructType([\n  StructField('firstname', StringType(), True),\n  StructField('middlename', StringType(), True),\n  StructField('lastname', StringType(), True)\n  ])\ndf = spark.createDataFrame(spark.sparkContext.emptyRDD(),schema)\ndf.printSchema()\n\ndf1 = spark.sparkContext.parallelize([]).toDF(schema)\ndf1.printSchema()\n\ndf2 = spark.createDataFrame([], schema)\ndf2.printSchema()\n\ndf3 = df3.filter(\"1=0\")\ndf3.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0b34fa17-59df-4971-9652-e7a58af9edc0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n\nroot\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n\nroot\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n\nroot\n |-- id: string (nullable = true)\n |-- input: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('pyspark-by-examples').getOrCreate()\n\narrayData = [\n        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n        ('Washington',None,None),\n        ('Jefferson',['1','2'],{})\n        ]\ndf = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\ndf.printSchema()\ndf.show()\n\nfrom pyspark.sql.functions import explode\ndf2 = df.select(df.name,explode(df.knownLanguages))\ndf2.printSchema()\ndf2.show()\n\nfrom pyspark.sql.functions import explode\ndf3 = df.select(df.name,explode(df.properties))\ndf3.printSchema()\ndf3.show()\n\nfrom pyspark.sql.functions import explode_outer\n\"\"\" with array \"\"\"\ndf.select(df.name,explode_outer(df.knownLanguages)).show()\n\"\"\" with map \"\"\"\ndf.select(df.name,explode_outer(df.properties)).show()\n\n\nfrom pyspark.sql.functions import posexplode\n\"\"\" with array \"\"\"\ndf.select(df.name,posexplode(df.knownLanguages)).show()\n\"\"\" with map \"\"\"\ndf.select(df.name,posexplode(df.properties)).show()\n\nfrom pyspark.sql.functions import posexplode_outer\n\"\"\" with array \"\"\"\ndf.select(df.name,posexplode_outer(df.knownLanguages)).show()\n\n\"\"\" with map \"\"\"\ndf.select(df.name,posexplode_outer(df.properties)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"12d5eed8-211e-44d7-a6d1-b72807b8e443","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- knownLanguages: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+----------+-------------------+--------------------+\n|      name|     knownLanguages|          properties|\n+----------+-------------------+--------------------+\n|     James|      [Java, Scala]|{eye -> brown, ha...|\n|   Michael|[Spark, Java, null]|{eye -> null, hai...|\n|    Robert|         [CSharp, ]|{eye -> , hair ->...|\n|Washington|               null|                null|\n| Jefferson|             [1, 2]|                  {}|\n+----------+-------------------+--------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- col: string (nullable = true)\n\n+---------+------+\n|     name|   col|\n+---------+------+\n|    James|  Java|\n|    James| Scala|\n|  Michael| Spark|\n|  Michael|  Java|\n|  Michael|  null|\n|   Robert|CSharp|\n|   Robert|      |\n|Jefferson|     1|\n|Jefferson|     2|\n+---------+------+\n\nroot\n |-- name: string (nullable = true)\n |-- key: string (nullable = false)\n |-- value: string (nullable = true)\n\n+-------+----+-----+\n|   name| key|value|\n+-------+----+-----+\n|  James| eye|brown|\n|  James|hair|black|\n|Michael| eye| null|\n|Michael|hair|brown|\n| Robert| eye|     |\n| Robert|hair|  red|\n+-------+----+-----+\n\n+----------+------+\n|      name|   col|\n+----------+------+\n|     James|  Java|\n|     James| Scala|\n|   Michael| Spark|\n|   Michael|  Java|\n|   Michael|  null|\n|    Robert|CSharp|\n|    Robert|      |\n|Washington|  null|\n| Jefferson|     1|\n| Jefferson|     2|\n+----------+------+\n\n+----------+----+-----+\n|      name| key|value|\n+----------+----+-----+\n|     James| eye|brown|\n|     James|hair|black|\n|   Michael| eye| null|\n|   Michael|hair|brown|\n|    Robert| eye|     |\n|    Robert|hair|  red|\n|Washington|null| null|\n| Jefferson|null| null|\n+----------+----+-----+\n\n+---------+---+------+\n|     name|pos|   col|\n+---------+---+------+\n|    James|  0|  Java|\n|    James|  1| Scala|\n|  Michael|  0| Spark|\n|  Michael|  1|  Java|\n|  Michael|  2|  null|\n|   Robert|  0|CSharp|\n|   Robert|  1|      |\n|Jefferson|  0|     1|\n|Jefferson|  1|     2|\n+---------+---+------+\n\n+-------+---+----+-----+\n|   name|pos| key|value|\n+-------+---+----+-----+\n|  James|  0| eye|brown|\n|  James|  1|hair|black|\n|Michael|  0| eye| null|\n|Michael|  1|hair|brown|\n| Robert|  0| eye|     |\n| Robert|  1|hair|  red|\n+-------+---+----+-----+\n\n+----------+----+------+\n|      name| pos|   col|\n+----------+----+------+\n|     James|   0|  Java|\n|     James|   1| Scala|\n|   Michael|   0| Spark|\n|   Michael|   1|  Java|\n|   Michael|   2|  null|\n|    Robert|   0|CSharp|\n|    Robert|   1|      |\n|Washington|null|  null|\n| Jefferson|   0|     1|\n| Jefferson|   1|     2|\n+----------+----+------+\n\n+----------+----+----+-----+\n|      name| pos| key|value|\n+----------+----+----+-----+\n|     James|   0| eye|brown|\n|     James|   1|hair|black|\n|   Michael|   0| eye| null|\n|   Michael|   1|hair|brown|\n|    Robert|   0| eye|     |\n|    Robert|   1|hair|  red|\n|Washington|null|null| null|\n| Jefferson|null|null| null|\n+----------+----+----+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import explode, flatten\n\n\nspark = SparkSession.builder.appName('pyspark-by-examples').getOrCreate()\n\narrayArrayData = [\n  (\"James\",[[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]]),\n  (\"Michael\",[[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]]),\n  (\"Robert\",[[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"]])\n]\n\ndf = spark.createDataFrame(data=arrayArrayData, schema = ['name','subjects'])\ndf.printSchema()\ndf.show(truncate=False)\n\n\"\"\" \"\"\"\ndf.select(df.name,explode(df.subjects)).show(truncate=False)\n\n\"\"\" creates a single array from an array of arrays. \"\"\"\ndf.select(df.name,flatten(df.subjects)).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"8164b454-907d-4763-8fde-f3b14bbe2107","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- subjects: array (nullable = true)\n |    |-- element: array (containsNull = true)\n |    |    |-- element: string (containsNull = true)\n\n+-------+-----------------------------------+\n|name   |subjects                           |\n+-------+-----------------------------------+\n|James  |[[Java, Scala, C++], [Spark, Java]]|\n|Michael|[[Spark, Java, C++], [Spark, Java]]|\n|Robert |[[CSharp, VB], [Spark, Python]]    |\n+-------+-----------------------------------+\n\n+-------+------------------+\n|name   |col               |\n+-------+------------------+\n|James  |[Java, Scala, C++]|\n|James  |[Spark, Java]     |\n|Michael|[Spark, Java, C++]|\n|Michael|[Spark, Java]     |\n|Robert |[CSharp, VB]      |\n|Robert |[Spark, Python]   |\n+-------+------------------+\n\n+-------+-------------------------------+\n|name   |flatten(subjects)              |\n+-------+-------------------------------+\n|James  |[Java, Scala, C++, Spark, Java]|\n|Michael|[Spark, Java, C++, Spark, Java]|\n|Robert |[CSharp, VB, Spark, Python]    |\n+-------+-------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nfrom pyspark.sql.functions import expr\n#Concatenate columns\ndata=[(\"James\",\"Bond\"),(\"Scott\",\"Varsa\")] \ndf=spark.createDataFrame(data).toDF(\"col1\",\"col2\") \ndf.withColumn(\"Name\",expr(\" col1 ||','|| col2\")).show()\n\n#Using CASE WHEN sql expression\ndata = [(\"James\",\"M\"),(\"Michael\",\"F\"),(\"Jen\",\"\")]\ncolumns = [\"name\",\"gender\"]\ndf = spark.createDataFrame(data = data, schema = columns)\ndf2 = df.withColumn(\"gender\", expr(\"CASE WHEN gender = 'M' THEN 'Male' \" +\n           \"WHEN gender = 'F' THEN 'Female' ELSE 'unknown' END\"))\ndf2.show()\n\n#Add months from a value of another column\ndata=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)] \ndf=spark.createDataFrame(data).toDF(\"date\",\"increment\") \ndf.select(df.date,df.increment,\n     expr(\"add_months(date,increment)\")\n  .alias(\"inc_date\")).show()\n\n# Providing alias using 'as'\ndf.select(df.date,df.increment,\n     expr(\"\"\"add_months(date,increment) as inc_date\"\"\")\n  ).show()\n\n# Add\ndf.select(df.date,df.increment,\n     expr(\"increment + 5 as new_increment\")\n  ).show()\n\ndf.select(\"increment\",expr(\"cast(increment as string) as str_increment\")) \\\n  .printSchema()\n#Use expr()  to filter the rows\ndata=[(100,2),(200,3000),(500,500)] \ndf=spark.createDataFrame(data).toDF(\"col1\",\"col2\") \ndf.filter(expr(\"col1 == col2\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c1cab9df-2f20-4a1c-a2cf-b70038de533e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+-----+-----------+\n| col1| col2|       Name|\n+-----+-----+-----------+\n|James| Bond| James,Bond|\n|Scott|Varsa|Scott,Varsa|\n+-----+-----+-----------+\n\n+-------+-------+\n|   name| gender|\n+-------+-------+\n|  James|   Male|\n|Michael| Female|\n|    Jen|unknown|\n+-------+-------+\n\n+----------+---------+----------+\n|      date|increment|  inc_date|\n+----------+---------+----------+\n|2019-01-23|        1|2019-02-23|\n|2019-06-24|        2|2019-08-24|\n|2019-09-20|        3|2019-12-20|\n+----------+---------+----------+\n\n+----------+---------+----------+\n|      date|increment|  inc_date|\n+----------+---------+----------+\n|2019-01-23|        1|2019-02-23|\n|2019-06-24|        2|2019-08-24|\n|2019-09-20|        3|2019-12-20|\n+----------+---------+----------+\n\n+----------+---------+-------------+\n|      date|increment|new_increment|\n+----------+---------+-------------+\n|2019-01-23|        1|            6|\n|2019-06-24|        2|            7|\n|2019-09-20|        3|            8|\n+----------+---------+-------------+\n\nroot\n |-- increment: long (nullable = true)\n |-- str_increment: string (nullable = true)\n\n+----+----+\n|col1|col2|\n+----+----+\n| 500| 500|\n+----+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nspark: SparkSession = SparkSession.builder \\\n    .master(\"local[1]\") \\\n    .appName(\"SparkByExamples.com\") \\\n    .getOrCreate()\n\ndata = [\n    (\"James\",None,\"M\"),\n    (\"Anna\",\"NY\",\"F\"),\n    (\"Julia\",None,None)\n]\n\ncolumns = [\"name\",\"state\",\"gender\"]\ndf =spark.createDataFrame(data,columns)\n\ndf.printSchema()\ndf.show()\n\ndf.filter(\"state is NULL\").show()\ndf.filter(df.state.isNull()).show()\ndf.filter(col(\"state\").isNull()).show()\n\ndf.filter(\"state IS NULL AND gender IS NULL\").show()\ndf.filter(df.state.isNull() & df.gender.isNull()).show()\n\ndf.filter(\"state is not NULL\").show()\ndf.filter(\"NOT state is NULL\").show()\ndf.filter(df.state.isNotNull()).show()\ndf.filter(col(\"state\").isNotNull()).show()\ndf.na.drop(subset=[\"state\"]).show()\n\ndf.createOrReplaceTempView(\"DATA\")\nspark.sql(\"SELECT * FROM DATA where STATE IS NULL\").show()\nspark.sql(\"SELECT * FROM DATA where STATE IS NULL AND GENDER IS NULL\").show()\nspark.sql(\"SELECT * FROM DATA where STATE IS NOT NULL\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"2c8aec3c-6f8b-4be2-813b-4e8e6ba0b3d2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- state: string (nullable = true)\n |-- gender: string (nullable = true)\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n| Anna|   NY|     F|\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|Julia| null|  null|\n+-----+-----+------+\n\n+----+-----+------+\n|name|state|gender|\n+----+-----+------+\n|Anna|   NY|     F|\n+----+-----+------+\n\n+----+-----+------+\n|name|state|gender|\n+----+-----+------+\n|Anna|   NY|     F|\n+----+-----+------+\n\n+----+-----+------+\n|name|state|gender|\n+----+-----+------+\n|Anna|   NY|     F|\n+----+-----+------+\n\n+----+-----+------+\n|name|state|gender|\n+----+-----+------+\n|Anna|   NY|     F|\n+----+-----+------+\n\n+----+-----+------+\n|name|state|gender|\n+----+-----+------+\n|Anna|   NY|     F|\n+----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|Julia| null|  null|\n+-----+-----+------+\n\n+----+-----+------+\n|name|state|gender|\n+----+-----+------+\n|Anna|   NY|     F|\n+----+-----+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType, ArrayType\nfrom pyspark.sql.functions import col,array_contains\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\narrayStructureData = [\n        ((\"James\",\"\",\"Smith\"),[\"Java\",\"Scala\",\"C++\"],\"OH\",\"M\"),\n        ((\"Anna\",\"Rose\",\"\"),[\"Spark\",\"Java\",\"C++\"],\"NY\",\"F\"),\n        ((\"Julia\",\"\",\"Williams\"),[\"CSharp\",\"VB\"],\"OH\",\"F\"),\n        ((\"Maria\",\"Anne\",\"Jones\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n        ((\"Jen\",\"Mary\",\"Brown\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n        ((\"Mike\",\"Mary\",\"Williams\"),[\"Python\",\"VB\"],\"OH\",\"M\")\n        ]\n        \narrayStructureSchema = StructType([\n        StructField('name', StructType([\n             StructField('firstname', StringType(), True),\n             StructField('middlename', StringType(), True),\n             StructField('lastname', StringType(), True)\n             ])),\n         StructField('languages', ArrayType(StringType()), True),\n         StructField('state', StringType(), True),\n         StructField('gender', StringType(), True)\n         ])\n\n\ndf = spark.createDataFrame(data = arrayStructureData, schema = arrayStructureSchema)\ndf.printSchema()\ndf.show(truncate=False)\n\n#Equals\ndf.filter(df.state == \"OH\") \\\n    .show(truncate=False)\n\n#Not equals\ndf.filter(~(df.state == \"OH\")) \\\n    .show(truncate=False)\ndf.filter(df.state != \"OH\") \\\n    .show(truncate=False)    \n    \ndf.filter(col(\"state\") == \"OH\") \\\n    .show(truncate=False)    \n    \ndf.filter(\"gender  == 'M'\") \\\n    .show(truncate=False)    \n\ndf.filter(\"gender  <> 'M'\") \\\n    .show(truncate=False)    \n\n#IS IN\nli=[\"OH\",\"CA\",\"DE\"]\ndf.filter(df.state.isin(li)).show()\n#IS NOT IN\ndf.filter(~df.state.isin(li)).show()\n\ndf.filter( (df.state  == \"OH\") & (df.gender  == \"M\") ) \\\n    .show(truncate=False)        \n\ndf.filter(array_contains(df.languages,\"Java\")) \\\n    .show(truncate=False)        \n\ndf.filter(df.name.lastname == \"Williams\") \\\n    .show(truncate=False) \n\ndf.filter(df.state.startswith(\"N\")).show()\ndf.filter(df.state.endswith(\"H\")).show()\ndf.filter(df.state.like(\"N%\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"3065ad4f-83a9-4a7f-9507-1881d6f8b21f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- languages: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- state: string (nullable = true)\n |-- gender: string (nullable = true)\n\n+----------------------+------------------+-----+------+\n|name                  |languages         |state|gender|\n+----------------------+------------------+-----+------+\n|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n|{Anna, Rose, }        |[Spark, Java, C++]|NY   |F     |\n|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n|{Maria, Anne, Jones}  |[CSharp, VB]      |NY   |M     |\n|{Jen, Mary, Brown}    |[CSharp, VB]      |NY   |M     |\n|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n+----------------------+------------------+-----+------+\n\n+----------------------+------------------+-----+------+\n|name                  |languages         |state|gender|\n+----------------------+------------------+-----+------+\n|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n+----------------------+------------------+-----+------+\n\n+--------------------+------------------+-----+------+\n|name                |languages         |state|gender|\n+--------------------+------------------+-----+------+\n|{Anna, Rose, }      |[Spark, Java, C++]|NY   |F     |\n|{Maria, Anne, Jones}|[CSharp, VB]      |NY   |M     |\n|{Jen, Mary, Brown}  |[CSharp, VB]      |NY   |M     |\n+--------------------+------------------+-----+------+\n\n+--------------------+------------------+-----+------+\n|name                |languages         |state|gender|\n+--------------------+------------------+-----+------+\n|{Anna, Rose, }      |[Spark, Java, C++]|NY   |F     |\n|{Maria, Anne, Jones}|[CSharp, VB]      |NY   |M     |\n|{Jen, Mary, Brown}  |[CSharp, VB]      |NY   |M     |\n+--------------------+------------------+-----+------+\n\n+----------------------+------------------+-----+------+\n|name                  |languages         |state|gender|\n+----------------------+------------------+-----+------+\n|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n+----------------------+------------------+-----+------+\n\n+----------------------+------------------+-----+------+\n|name                  |languages         |state|gender|\n+----------------------+------------------+-----+------+\n|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n|{Maria, Anne, Jones}  |[CSharp, VB]      |NY   |M     |\n|{Jen, Mary, Brown}    |[CSharp, VB]      |NY   |M     |\n|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n+----------------------+------------------+-----+------+\n\n+-------------------+------------------+-----+------+\n|name               |languages         |state|gender|\n+-------------------+------------------+-----+------+\n|{Anna, Rose, }     |[Spark, Java, C++]|NY   |F     |\n|{Julia, , Williams}|[CSharp, VB]      |OH   |F     |\n+-------------------+------------------+-----+------+\n\n+--------------------+------------------+-----+------+\n|                name|         languages|state|gender|\n+--------------------+------------------+-----+------+\n|    {James, , Smith}|[Java, Scala, C++]|   OH|     M|\n| {Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n+--------------------+------------------+-----+------+\n\n+--------------------+------------------+-----+------+\n|                name|         languages|state|gender|\n+--------------------+------------------+-----+------+\n|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n+--------------------+------------------+-----+------+\n\n+----------------------+------------------+-----+------+\n|name                  |languages         |state|gender|\n+----------------------+------------------+-----+------+\n|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n+----------------------+------------------+-----+------+\n\n+----------------+------------------+-----+------+\n|name            |languages         |state|gender|\n+----------------+------------------+-----+------+\n|{James, , Smith}|[Java, Scala, C++]|OH   |M     |\n|{Anna, Rose, }  |[Spark, Java, C++]|NY   |F     |\n+----------------+------------------+-----+------+\n\n+----------------------+------------+-----+------+\n|name                  |languages   |state|gender|\n+----------------------+------------+-----+------+\n|{Julia, , Williams}   |[CSharp, VB]|OH   |F     |\n|{Mike, Mary, Williams}|[Python, VB]|OH   |M     |\n+----------------------+------------+-----+------+\n\n+--------------------+------------------+-----+------+\n|                name|         languages|state|gender|\n+--------------------+------------------+-----+------+\n|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n+--------------------+------------------+-----+------+\n\n+--------------------+------------------+-----+------+\n|                name|         languages|state|gender|\n+--------------------+------------------+-----+------+\n|    {James, , Smith}|[Java, Scala, C++]|   OH|     M|\n| {Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n+--------------------+------------------+-----+------+\n\n+--------------------+------------------+-----+------+\n|                name|         languages|state|gender|\n+--------------------+------------------+-----+------+\n|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n+--------------------+------------------+-----+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndata2 = [(1,\"James Smith\"), (2,\"Michael Rose\"),\n    (3,\"Robert Williams\"), (4,\"Rames Rose\"),(5,\"Rames rose\")\n  ]\ndf2 = spark.createDataFrame(data = data2, schema = [\"id\",\"name\"])\n\ndf2.filter(df2.name.like(\"%rose%\")).show()\ndf2.filter(df2.name.rlike(\"(?i)^*rose$\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c6617953-8dc2-48cc-af8d-5e7d7044d024","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----------+\n| id|      name|\n+---+----------+\n|  5|Rames rose|\n+---+----------+\n\n+---+------------+\n| id|        name|\n+---+------------+\n|  2|Michael Rose|\n|  4|  Rames Rose|\n|  5|  Rames rose|\n+---+------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nspark = SparkSession.builder \\\n    .master(\"local[1]\") \\\n    .appName(\"SparkByExamples.com\") \\\n    .getOrCreate()\n\ndata = [\n    (\"James\",None,\"M\"),\n    (\"Anna\",\"NY\",\"F\"),\n    (\"Julia\",None,None)\n  ]\n\ncolumns = [\"name\",\"state\",\"gender\"]\ndf = spark.createDataFrame(data,columns)\ndf.show()\n\ndf.filter(\"state is NULL\").show()\ndf.filter(df.state.isNull()).show()\ndf.filter(col(\"state\").isNull()).show() \n\ndf.na.drop(\"any\", subset=[\"state\"]).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0faea22f-6c6c-457e-b9ff-9ad29339ec5d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n| Anna|   NY|     F|\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n|Julia| null|  null|\n+-----+-----+------+\n\n+----+-----+------+\n|name|state|gender|\n+----+-----+------+\n|Anna|   NY|     F|\n+----+-----+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col,sum,avg,max\n\nspark = SparkSession.builder \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\nsimpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n    (\"Michael\",\"Sales\",\"NV\",86000,56,20000),\n    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n    (\"Raman\",\"Finance\",\"DE\",99000,40,24000),\n    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n    (\"Jeff\",\"Marketing\",\"NV\",80000,25,18000),\n    (\"Kumar\",\"Marketing\",\"NJ\",91000,50,21000)\n  ]\n\nschema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\ndf = spark.createDataFrame(data=simpleData, schema = schema)\ndf.printSchema()\ndf.show(truncate=False)\n\ndf.groupBy(\"state\").sum(\"salary\").show()\n\ndfGroup=df.groupBy(\"state\") \\\n          .agg(sum(\"salary\").alias(\"sum_salary\"))\n          \ndfGroup.show(truncate=False)\n\ndfFilter=dfGroup.filter(dfGroup.sum_salary > 100000)\ndfFilter.show()\n\nfrom pyspark.sql.functions import asc\ndfFilter.sort(\"sum_salary\").show()\n\nfrom pyspark.sql.functions import desc\ndfFilter.sort(desc(\"sum_salary\")).show()\n\ndf.groupBy(\"state\") \\\n  .agg(sum(\"salary\").alias(\"sum_salary\")) \\\n  .filter(col(\"sum_salary\") > 100000)  \\\n  .sort(desc(\"sum_salary\")) \\\n  .show()\n  \ndf.createOrReplaceTempView(\"EMP\")\nspark.sql(\"select state, sum(salary) as sum_salary from EMP \" +\n          \"group by state having sum_salary > 100000 \" + \n          \"order by sum_salary desc\").show()\n\ndf.groupBy(\"state\") \\\n  .sum(\"salary\") \\\n  .withColumnRenamed(\"sum(salary)\", \"sum_salary\") \\\n  .show()\n\ndf.groupBy(\"state\") \\\n  .sum(\"salary\") \\\n  .select(col(\"state\"),col(\"sum(salary)\").alias(\"sum_salary\")) \\\n  .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"65e40255-c08c-4158-bc0b-855bace44160","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- state: string (nullable = true)\n |-- salary: long (nullable = true)\n |-- age: long (nullable = true)\n |-- bonus: long (nullable = true)\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|James        |Sales     |NY   |90000 |34 |10000|\n|Michael      |Sales     |NV   |86000 |56 |20000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Raman        |Finance   |DE   |99000 |40 |24000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |NV   |80000 |25 |18000|\n|Kumar        |Marketing |NJ   |91000 |50 |21000|\n+-------------+----------+-----+------+---+-----+\n\n+-----+-----------+\n|state|sum(salary)|\n+-----+-----------+\n|   NY|     252000|\n|   NV|     166000|\n|   CA|     171000|\n|   DE|      99000|\n|   NJ|      91000|\n+-----+-----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|NY   |252000    |\n|NV   |166000    |\n|CA   |171000    |\n|DE   |99000     |\n|NJ   |91000     |\n+-----+----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|   NY|    252000|\n|   NV|    166000|\n|   CA|    171000|\n+-----+----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|   NV|    166000|\n|   CA|    171000|\n|   NY|    252000|\n+-----+----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|   NY|    252000|\n|   CA|    171000|\n|   NV|    166000|\n+-----+----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|   NY|    252000|\n|   CA|    171000|\n|   NV|    166000|\n+-----+----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|   NY|    252000|\n|   CA|    171000|\n|   NV|    166000|\n+-----+----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|   NY|    252000|\n|   NV|    166000|\n|   CA|    171000|\n|   DE|     99000|\n|   NJ|     91000|\n+-----+----------+\n\n+-----+----------+\n|state|sum_salary|\n+-----+----------+\n|   NY|    252000|\n|   NV|    166000|\n|   CA|    171000|\n|   DE|     99000|\n|   NJ|     91000|\n+-----+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col,sum,avg,max\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nsimpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n  ]\n\nschema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\ndf = spark.createDataFrame(data=simpleData, schema = schema)\ndf.printSchema()\ndf.show(truncate=False)\n\ndf.groupBy(\"department\").sum(\"salary\").show(truncate=False)\n\ndf.groupBy(\"department\").count().show(truncate=False)\n\n\ndf.groupBy(\"department\",\"state\") \\\n    .sum(\"salary\",\"bonus\") \\\n   .show(truncate=False)\n\ndf.groupBy(\"department\") \\\n    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n         avg(\"salary\").alias(\"avg_salary\"), \\\n         sum(\"bonus\").alias(\"sum_bonus\"), \\\n         max(\"bonus\").alias(\"max_bonus\") \\\n     ) \\\n    .show(truncate=False)\n    \ndf.groupBy(\"department\") \\\n    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n      avg(\"salary\").alias(\"avg_salary\"), \\\n      sum(\"bonus\").alias(\"sum_bonus\"), \\\n      max(\"bonus\").alias(\"max_bonus\")) \\\n    .where(col(\"sum_bonus\") >= 50000) \\\n    .show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6a62a29d-8728-4ad6-81ad-e6f10246341a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- state: string (nullable = true)\n |-- salary: long (nullable = true)\n |-- age: long (nullable = true)\n |-- bonus: long (nullable = true)\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|James        |Sales     |NY   |90000 |34 |10000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n+-------------+----------+-----+------+---+-----+\n\n+----------+-----------+\n|department|sum(salary)|\n+----------+-----------+\n|Sales     |257000     |\n|Finance   |351000     |\n|Marketing |171000     |\n+----------+-----------+\n\n+----------+-----+\n|department|count|\n+----------+-----+\n|Sales     |3    |\n|Finance   |4    |\n|Marketing |2    |\n+----------+-----+\n\n+----------+-----+-----------+----------+\n|department|state|sum(salary)|sum(bonus)|\n+----------+-----+-----------+----------+\n|Sales     |NY   |176000     |30000     |\n|Sales     |CA   |81000      |23000     |\n|Finance   |CA   |189000     |47000     |\n|Finance   |NY   |162000     |34000     |\n|Marketing |NY   |91000      |21000     |\n|Marketing |CA   |80000      |18000     |\n+----------+-----+-----------+----------+\n\n+----------+----------+-----------------+---------+---------+\n|department|sum_salary|avg_salary       |sum_bonus|max_bonus|\n+----------+----------+-----------------+---------+---------+\n|Sales     |257000    |85666.66666666667|53000    |23000    |\n|Finance   |351000    |87750.0          |81000    |24000    |\n|Marketing |171000    |85500.0          |39000    |21000    |\n+----------+----------+-----------------+---------+---------+\n\n+----------+----------+-----------------+---------+---------+\n|department|sum_salary|avg_salary       |sum_bonus|max_bonus|\n+----------+----------+-----------------+---------+---------+\n|Sales     |257000    |85666.66666666667|53000    |23000    |\n|Finance   |351000    |87750.0          |81000    |24000    |\n+----------+----------+-----------------+---------+---------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\n# Create SparkSession\nspark = SparkSession.builder \\\n          .appName('SparkByExamples.com') \\\n          .getOrCreate()\n#EMP DataFrame\nempData = [(1,\"Smith\",10), (2,\"Rose\",20),\n    (3,\"Williams\",10), (4,\"Jones\",30)\n  ]\nempColumns = [\"emp_id\",\"name\",\"emp_dept_id\"]\nempDF = spark.createDataFrame(empData,empColumns)\nempDF.show()\n\n#DEPT DataFrame\ndeptData = [(\"Finance\",10), (\"Marketing\",20),\n    (\"Sales\",30),(\"IT\",40)\n  ]\ndeptColumns = [\"dept_name\",\"dept_id\"]\ndeptDF=spark.createDataFrame(deptData,deptColumns)  \ndeptDF.show()\n\n#Address DataFrame\naddData=[(1,\"1523 Main St\",\"SFO\",\"CA\"),\n    (2,\"3453 Orange St\",\"SFO\",\"NY\"),\n    (3,\"34 Warner St\",\"Jersey\",\"NJ\"),\n    (4,\"221 Cavalier St\",\"Newark\",\"DE\"),\n    (5,\"789 Walnut St\",\"Sandiago\",\"CA\")\n  ]\naddColumns = [\"emp_id\",\"addline1\",\"city\",\"state\"]\naddDF = spark.createDataFrame(addData,addColumns)\naddDF.show()\n\n#Join two DataFrames\nempDF.join(addDF,empDF[\"emp_id\"] == addDF[\"emp_id\"]).show()\n\n#Drop duplicate column\nempDF.join(addDF,[\"emp_id\"]).show()\n\n#Join Multiple DataFrames\nempDF.join(addDF,[\"emp_id\"]) \\\n     .join(deptDF,empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"]) \\\n     .show()\n\n#Using Where for Join Condition\nempDF.join(deptDF).where(empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"]) \\\n    .join(addDF).where(empDF[\"emp_id\"] == addDF[\"emp_id\"]) \\\n    .show()\n    \n#SQL\nempDF.createOrReplaceTempView(\"EMP\")\ndeptDF.createOrReplaceTempView(\"DEPT\")\naddDF.createOrReplaceTempView(\"ADD\")\n\nspark.sql(\"select * from EMP e, DEPT d, ADD a \" + \\\n    \"where e.emp_dept_id == d.dept_id and e.emp_id == a.emp_id\") \\\n    .show()\n    \n#\ndf1 = spark.createDataFrame(\n    [(1, \"A\"), (2, \"B\"), (3, \"C\")],\n    [\"A1\", \"A2\"])\n\ndf2 = spark.createDataFrame(\n    [(1, \"F\"), (2, \"B\")], \n    [\"B1\", \"B2\"])\n\ndf = df1.join(df2, (df1.A1 == df2.B1) & (df1.A2 == df2.B2))\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"9f3a7c31-f8fe-43be-a905-486c36460b59","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------+--------+-----------+\n|emp_id|    name|emp_dept_id|\n+------+--------+-----------+\n|     1|   Smith|         10|\n|     2|    Rose|         20|\n|     3|Williams|         10|\n|     4|   Jones|         30|\n+------+--------+-----------+\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|  Finance|     10|\n|Marketing|     20|\n|    Sales|     30|\n|       IT|     40|\n+---------+-------+\n\n+------+---------------+--------+-----+\n|emp_id|       addline1|    city|state|\n+------+---------------+--------+-----+\n|     1|   1523 Main St|     SFO|   CA|\n|     2| 3453 Orange St|     SFO|   NY|\n|     3|   34 Warner St|  Jersey|   NJ|\n|     4|221 Cavalier St|  Newark|   DE|\n|     5|  789 Walnut St|Sandiago|   CA|\n+------+---------------+--------+-----+\n\n+------+--------+-----------+------+---------------+------+-----+\n|emp_id|    name|emp_dept_id|emp_id|       addline1|  city|state|\n+------+--------+-----------+------+---------------+------+-----+\n|     1|   Smith|         10|     1|   1523 Main St|   SFO|   CA|\n|     2|    Rose|         20|     2| 3453 Orange St|   SFO|   NY|\n|     3|Williams|         10|     3|   34 Warner St|Jersey|   NJ|\n|     4|   Jones|         30|     4|221 Cavalier St|Newark|   DE|\n+------+--------+-----------+------+---------------+------+-----+\n\n+------+--------+-----------+---------------+------+-----+\n|emp_id|    name|emp_dept_id|       addline1|  city|state|\n+------+--------+-----------+---------------+------+-----+\n|     1|   Smith|         10|   1523 Main St|   SFO|   CA|\n|     2|    Rose|         20| 3453 Orange St|   SFO|   NY|\n|     3|Williams|         10|   34 Warner St|Jersey|   NJ|\n|     4|   Jones|         30|221 Cavalier St|Newark|   DE|\n+------+--------+-----------+---------------+------+-----+\n\n+------+--------+-----------+---------------+------+-----+---------+-------+\n|emp_id|    name|emp_dept_id|       addline1|  city|state|dept_name|dept_id|\n+------+--------+-----------+---------------+------+-----+---------+-------+\n|     3|Williams|         10|   34 Warner St|Jersey|   NJ|  Finance|     10|\n|     1|   Smith|         10|   1523 Main St|   SFO|   CA|  Finance|     10|\n|     2|    Rose|         20| 3453 Orange St|   SFO|   NY|Marketing|     20|\n|     4|   Jones|         30|221 Cavalier St|Newark|   DE|    Sales|     30|\n+------+--------+-----------+---------------+------+-----+---------+-------+\n\n+------+--------+-----------+---------+-------+------+---------------+------+-----+\n|emp_id|    name|emp_dept_id|dept_name|dept_id|emp_id|       addline1|  city|state|\n+------+--------+-----------+---------+-------+------+---------------+------+-----+\n|     1|   Smith|         10|  Finance|     10|     1|   1523 Main St|   SFO|   CA|\n|     2|    Rose|         20|Marketing|     20|     2| 3453 Orange St|   SFO|   NY|\n|     3|Williams|         10|  Finance|     10|     3|   34 Warner St|Jersey|   NJ|\n|     4|   Jones|         30|    Sales|     30|     4|221 Cavalier St|Newark|   DE|\n+------+--------+-----------+---------+-------+------+---------------+------+-----+\n\n+------+--------+-----------+---------+-------+------+---------------+------+-----+\n|emp_id|    name|emp_dept_id|dept_name|dept_id|emp_id|       addline1|  city|state|\n+------+--------+-----------+---------+-------+------+---------------+------+-----+\n|     1|   Smith|         10|  Finance|     10|     1|   1523 Main St|   SFO|   CA|\n|     2|    Rose|         20|Marketing|     20|     2| 3453 Orange St|   SFO|   NY|\n|     3|Williams|         10|  Finance|     10|     3|   34 Warner St|Jersey|   NJ|\n|     4|   Jones|         30|    Sales|     30|     4|221 Cavalier St|Newark|   DE|\n+------+--------+-----------+---------+-------+------+---------------+------+-----+\n\n+---+---+---+---+\n| A1| A2| B1| B2|\n+---+---+---+---+\n|  2|  B|  2|  B|\n+---+---+---+---+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nemp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n  ]\nempColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n       \"emp_dept_id\",\"gender\",\"salary\"]\n\nempDF = spark.createDataFrame(data=emp, schema = empColumns)\nempDF.printSchema()\nempDF.show(truncate=False)\n\n\ndept = [(\"Finance\",10), \\\n    (\"Marketing\",20), \\\n    (\"Sales\",30), \\\n    (\"IT\",40) \\\n  ]\ndeptColumns = [\"dept_name\",\"dept_id\"]\ndeptDF = spark.createDataFrame(data=dept, schema = deptColumns)\ndeptDF.printSchema()\ndeptDF.show(truncate=False)\n  \nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\") \\\n     .show(truncate=False)\n\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"outer\") \\\n    .show(truncate=False)\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"full\") \\\n    .show(truncate=False)\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"fullouter\") \\\n    .show(truncate=False)\n    \nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\") \\\n    .show(truncate=False)\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftouter\") \\\n   .show(truncate=False)\n\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"right\") \\\n   .show(truncate=False)\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"rightouter\") \\\n   .show(truncate=False)\n\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftsemi\") \\\n   .show(truncate=False)\n   \nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftanti\") \\\n   .show(truncate=False)\n   \nempDF.alias(\"emp1\").join(empDF.alias(\"emp2\"), \\\n    col(\"emp1.superior_emp_id\") == col(\"emp2.emp_id\"),\"inner\") \\\n    .select(col(\"emp1.emp_id\"),col(\"emp1.name\"), \\\n      col(\"emp2.emp_id\").alias(\"superior_emp_id\"), \\\n      col(\"emp2.name\").alias(\"superior_emp_name\")) \\\n   .show(truncate=False)\n\nempDF.createOrReplaceTempView(\"EMP\")\ndeptDF.createOrReplaceTempView(\"DEPT\")\n   \njoinDF = spark.sql(\"select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id\") \\\n  .show(truncate=False)\n\njoinDF2 = spark.sql(\"select * from EMP e INNER JOIN DEPT d ON e.emp_dept_id == d.dept_id\") \\\n  .show(truncate=False)\n  "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1d72ed2a-219b-4c26-95d4-615842bc193f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- emp_id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- superior_emp_id: long (nullable = true)\n |-- year_joined: string (nullable = true)\n |-- emp_dept_id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |\n|2     |Rose    |1              |2010       |20         |M     |4000  |\n|3     |Williams|1              |2010       |10         |M     |1000  |\n|4     |Jones   |2              |2005       |10         |F     |2000  |\n|5     |Brown   |2              |2010       |40         |      |-1    |\n|6     |Brown   |2              |2010       |50         |      |-1    |\n+------+--------+---------------+-----------+-----------+------+------+\n\nroot\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |\n|3     |Williams|1              |2010       |10         |M     |1000  |\n|4     |Jones   |2              |2005       |10         |F     |2000  |\n|2     |Rose    |1              |2010       |20         |M     |4000  |\n|5     |Brown   |2              |2010       |40         |      |-1    |\n+------+--------+---------------+-----------+-----------+------+------+\n\n+------+-----+---------------+-----------+-----------+------+------+\n|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+-----+---------------+-----------+-----------+------+------+\n|6     |Brown|2              |2010       |50         |      |-1    |\n+------+-----+---------------+-----------+-----------+------+------+\n\n+------+--------+---------------+-----------------+\n|emp_id|name    |superior_emp_id|superior_emp_name|\n+------+--------+---------------+-----------------+\n|2     |Rose    |1              |Smith            |\n|3     |Williams|1              |Smith            |\n|4     |Jones   |2              |Rose             |\n|5     |Brown   |2              |Rose             |\n|6     |Brown   |2              |Rose             |\n+------+--------+---------------+-----------------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('machinelearninggeeks.com').getOrCreate()\n  \nemp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n  ]\nempColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n       \"emp_dept_id\",\"gender\",\"salary\"]\n\nempDF = spark.createDataFrame(data=emp, schema = empColumns)\nempDF.printSchema()\nempDF.show(truncate=False)\n\ndept = [(\"Finance\",10), \\\n    (\"Marketing\",20), \\\n    (\"Sales\",30), \\\n    (\"IT\",40) \\\n  ]\ndeptColumns = [\"dept_name\",\"dept_id\"]\ndeptDF = spark.createDataFrame(data=dept, schema = deptColumns)\ndeptDF.printSchema()\ndeptDF.show(truncate=False)\n\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\").show(truncate=False)\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftouter\").show(truncate=False)\n\nempDF.createOrReplaceTempView(\"EMP\")\ndeptDF.createOrReplaceTempView(\"DEPT\")\n\njoinDF2 = spark.sql(\"SELECT e.* FROM EMP e LEFT ANTI JOIN DEPT d ON e.emp_dept_id == d.dept_id\") \\\n  .show(truncate=False)    "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"886ab134-16f4-4af0-b5ab-a544d4a24f76","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- emp_id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- superior_emp_id: long (nullable = true)\n |-- year_joined: string (nullable = true)\n |-- emp_dept_id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |\n|2     |Rose    |1              |2010       |20         |M     |4000  |\n|3     |Williams|1              |2010       |10         |M     |1000  |\n|4     |Jones   |2              |2005       |10         |F     |2000  |\n|5     |Brown   |2              |2010       |40         |      |-1    |\n|6     |Brown   |2              |2010       |50         |      |-1    |\n+------+--------+---------------+-----------+-----------+------+------+\n\nroot\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+-----+---------------+-----------+-----------+------+------+\n|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+-----+---------------+-----------+-----------+------+------+\n|6     |Brown|2              |2010       |50         |      |-1    |\n+------+-----+---------------+-----------+-----------+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, lit, when\n\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndata = [(\"111\",50000),(\"222\",60000),(\"333\",40000)]\ncolumns= [\"EmpId\",\"Salary\"]\ndf = spark.createDataFrame(data = data, schema = columns)\ndf.printSchema()\ndf.show(truncate=False)\n\nfrom pyspark.sql.functions import col,lit\ndf2 = df.select(col(\"EmpId\"),col(\"Salary\"),lit(\"1\").alias(\"lit_value1\"))\ndf2.show(truncate=False)\n\n\nfrom pyspark.sql.functions import when\ndf3 = df2.withColumn(\"lit_value2\", when((col(\"Salary\") >=40000) & (col(\"Salary\") <= 50000),lit(\"100\")).otherwise(lit(\"200\")))\ndf3.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0d3fae56-c22a-47af-8822-c03827e81d0b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- EmpId: string (nullable = true)\n |-- Salary: long (nullable = true)\n\n+-----+------+\n|EmpId|Salary|\n+-----+------+\n|111  |50000 |\n|222  |60000 |\n|333  |40000 |\n+-----+------+\n\n+-----+------+----------+\n|EmpId|Salary|lit_value1|\n+-----+------+----------+\n|111  |50000 |1         |\n|222  |60000 |1         |\n|333  |40000 |1         |\n+-----+------+----------+\n\n+-----+------+----------+----------+\n|EmpId|Salary|lit_value1|lit_value2|\n+-----+------+----------+----------+\n|111  |50000 |1         |100       |\n|222  |60000 |1         |200       |\n|333  |40000 |1         |100       |\n+-----+------+----------+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndata = [('James','Smith','M',3000),\n  ('Anna','Rose','F',4100),\n  ('Robert','Williams','M',6200), \n]\n\ncolumns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\ndf = spark.createDataFrame(data=data, schema = columns)\ndf.show()\n\n#Example 1 mapPartitions()\ndef reformat(partitionData):\n    for row in partitionData:\n        yield [row.firstname+\",\"+row.lastname,row.salary*10/100]\ndf.rdd.mapPartitions(reformat).toDF().show()\n\n#Example 2 mapPartitions()\ndef reformat2(partitionData):\n  updatedData = []\n  for row in partitionData:\n    name=row.firstname+\",\"+row.lastname\n    bonus=row.salary*10/100\n    updatedData.append([name,bonus])\n  return iter(updatedData)\n\ndf2=df.rdd.mapPartitions(reformat2).toDF([\"name\",\"bonus\"])\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4c797cfd-7567-497b-85d3-f64926e28c59","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+--------+------+------+\n|firstname|lastname|gender|salary|\n+---------+--------+------+------+\n|    James|   Smith|     M|  3000|\n|     Anna|    Rose|     F|  4100|\n|   Robert|Williams|     M|  6200|\n+---------+--------+------+------+\n\n+---------------+-----+\n|             _1|   _2|\n+---------------+-----+\n|    James,Smith|300.0|\n|      Anna,Rose|410.0|\n|Robert,Williams|620.0|\n+---------------+-----+\n\n+---------------+-----+\n|           name|bonus|\n+---------------+-----+\n|    James,Smith|300.0|\n|      Anna,Rose|410.0|\n|Robert,Williams|620.0|\n+---------------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndataDictionary = [\n        ('James',{'hair':'black','eye':'brown'}),\n        ('Michael',{'hair':'brown','eye':None}),\n        ('Robert',{'hair':'red','eye':'black'}),\n        ('Washington',{'hair':'grey','eye':'grey'}),\n        ('Jefferson',{'hair':'brown','eye':''})\n        ]\n\n# Using StructType schema\nfrom pyspark.sql.types import StructField, StructType, StringType, MapType\nschema = StructType([\n    StructField('name', StringType(), True),\n    StructField('properties', MapType(StringType(),StringType()),True)\n])\ndf = spark.createDataFrame(data=dataDictionary, schema = schema)\ndf.printSchema()\ndf.show(truncate=False)\n\ndf3=df.rdd.map(lambda x: \\\n    (x.name,x.properties[\"hair\"],x.properties[\"eye\"])) \\\n    .toDF([\"name\",\"hair\",\"eye\"])\ndf3.printSchema()\ndf3.show()\n\ndf.withColumn(\"hair\",df.properties.getItem(\"hair\")) \\\n  .withColumn(\"eye\",df.properties.getItem(\"eye\")) \\\n  .drop(\"properties\") \\\n  .show()\n\ndf.withColumn(\"hair\",df.properties[\"hair\"]) \\\n  .withColumn(\"eye\",df.properties[\"eye\"]) \\\n  .drop(\"properties\") \\\n  .show()\n\nfrom pyspark.sql.functions import explode\ndf.select(df.name,explode(df.properties)).show()\n\nfrom pyspark.sql.functions import map_keys\ndf.select(df.name,map_keys(df.properties)).show()\n\nfrom pyspark.sql.functions import map_values\ndf.select(df.name,map_values(df.properties)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4d04b624-e555-435f-85c0-53dcca10e3ff","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+----------+-----------------------------+\n|name      |properties                   |\n+----------+-----------------------------+\n|James     |{eye -> brown, hair -> black}|\n|Michael   |{eye -> null, hair -> brown} |\n|Robert    |{eye -> black, hair -> red}  |\n|Washington|{eye -> grey, hair -> grey}  |\n|Jefferson |{eye -> , hair -> brown}     |\n+----------+-----------------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- hair: string (nullable = true)\n |-- eye: string (nullable = true)\n\n+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n+----------+----+-----+\n|      name| key|value|\n+----------+----+-----+\n|     James| eye|brown|\n|     James|hair|black|\n|   Michael| eye| null|\n|   Michael|hair|brown|\n|    Robert| eye|black|\n|    Robert|hair|  red|\n|Washington| eye| grey|\n|Washington|hair| grey|\n| Jefferson| eye|     |\n| Jefferson|hair|brown|\n+----------+----+-----+\n\n+----------+--------------------+\n|      name|map_keys(properties)|\n+----------+--------------------+\n|     James|         [eye, hair]|\n|   Michael|         [eye, hair]|\n|    Robert|         [eye, hair]|\n|Washington|         [eye, hair]|\n| Jefferson|         [eye, hair]|\n+----------+--------------------+\n\n+----------+----------------------+\n|      name|map_values(properties)|\n+----------+----------------------+\n|     James|        [brown, black]|\n|   Michael|         [null, brown]|\n|    Robert|          [black, red]|\n|Washington|          [grey, grey]|\n| Jefferson|             [, brown]|\n+----------+----------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col,sum,avg,max\n\nspark = SparkSession.builder \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\nsimpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n    (\"Michael\",\"Sales\",\"NV\",86000,56,20000),\n    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n    (\"Raman\",\"Finance\",\"DE\",99000,40,24000),\n    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n    (\"Jeff\",\"Marketing\",\"NV\",80000,25,18000),\n    (\"Kumar\",\"Marketing\",\"NJ\",91000,50,21000)\n  ]\n\nschema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\ndf = spark.createDataFrame(data=simpleData, schema = schema)\ndf.printSchema()\ndf.show(truncate=False)\n\ndfSort=df.sort(df.state,df.salary).groupBy(df.state).agg(sum(df.salary))\ndfSort.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"8b2674a2-cf86-45c7-8d05-f566746d09e7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- state: string (nullable = true)\n |-- salary: long (nullable = true)\n |-- age: long (nullable = true)\n |-- bonus: long (nullable = true)\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|James        |Sales     |NY   |90000 |34 |10000|\n|Michael      |Sales     |NV   |86000 |56 |20000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Raman        |Finance   |DE   |99000 |40 |24000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |NV   |80000 |25 |18000|\n|Kumar        |Marketing |NJ   |91000 |50 |21000|\n+-------------+----------+-----+------+---+-----+\n\n+-----+-----------+\n|state|sum(salary)|\n+-----+-----------+\n|   NY|     252000|\n|   NV|     166000|\n|   CA|     171000|\n|   DE|      99000|\n|   NJ|      91000|\n+-----+-----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nsimpleData = [\n    (\"James\", \"Sales\", \"NY\", 90000, 34, 10000),\n    (\"Michael\", \"Sales\", \"NY\", 86000, 56, 20000),\n    (\"Robert\", \"Sales\", \"CA\", 81000, 30, 23000),\n    (\"Maria\", \"Finance\", \"CA\", 90000, 24, 23000),\n    (\"Raman\", \"Finance\", \"CA\", 99000, 40, 24000),\n    (\"Scott\", \"Finance\", \"NY\", 83000, 36, 19000),\n    (\"Jen\", \"Finance\", \"NY\", 79000, 53, 15000),\n    (\"Jeff\", \"Marketing\", \"CA\", 80000, 25, 18000),\n    (\"Kumar\", \"Marketing\", \"NY\", 91000, 50, 21000)\n]\ncolumns = [\"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\"]\n\ndf = spark.createDataFrame(data=simpleData, schema=columns)\n\ndf.printSchema()\ndf.show(truncate=False)\n\ndf.sort(col(\"department\"), col(\"state\")).show(truncate=False)\n\ndf.sort(col(\"department\").asc(), col(\"state\").asc()).show(truncate=False)\n\ndf.sort(col(\"department\").asc(), col(\"state\").desc()).show(truncate=False)\n\ndf.createOrReplaceTempView(\"EMP\")\nspark.sql(\"SELECT employee_name, department, state, salary, age, bonus FROM EMP ORDER BY department ASC\").show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"2501b95b-6ff3-4f27-bb6c-c42f9cd6f8b5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- state: string (nullable = true)\n |-- salary: long (nullable = true)\n |-- age: long (nullable = true)\n |-- bonus: long (nullable = true)\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|James        |Sales     |NY   |90000 |34 |10000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|James        |Sales     |NY   |90000 |34 |10000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|James        |Sales     |NY   |90000 |34 |10000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|James        |Sales     |NY   |90000 |34 |10000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|James        |Sales     |NY   |90000 |34 |10000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n+-------------+----------+-----+------+---+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\nrdd=spark.sparkContext.parallelize([1,2,3,4,5])\n\nrddCollect = rdd.collect()\nprint(\"Number of Partitions: \"+str(rdd.getNumPartitions()))\nprint(\"Action: First element: \"+str(rdd.first()))\nprint(rddCollect)\n\nemptyRDD = spark.sparkContext.emptyRDD()\nemptyRDD2 = rdd=spark.sparkContext.parallelize([])\n\nprint(\"\"+str(emptyRDD2.isEmpty()))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ef3aea34-c097-4355-abf7-cedc702bb34a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Number of Partitions: 8\nAction: First element: 1\n[1, 2, 3, 4, 5]\nTrue\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import expr\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndata = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]\n\ncolumns= [\"Product\",\"Amount\",\"Country\"]\ndf = spark.createDataFrame(data = data, schema = columns)\ndf.printSchema()\ndf.show(truncate=False)\n\npivotDF = df.groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\")\npivotDF.printSchema()\npivotDF.show(truncate=False)\n\npivotDF = df.groupBy(\"Product\",\"Country\") \\\n      .sum(\"Amount\") \\\n      .groupBy(\"Product\") \\\n      .pivot(\"Country\") \\\n      .sum(\"sum(Amount)\")\npivotDF.printSchema()\npivotDF.show(truncate=False)\n\n\n\"\"\" unpivot \"\"\"\n\"\"\" unpivot \"\"\"\nunpivotExpr = \"stack(3, 'Canada', Canada, 'China', China, 'Mexico', Mexico) as (Country,Total)\"\nunPivotDF = pivotDF.select(\"Product\", expr(unpivotExpr)) \\\n    .where(\"Total is not null\")\nunPivotDF.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"7d8ff708-2664-4348-aa2b-c182b5428b08","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- Product: string (nullable = true)\n |-- Amount: long (nullable = true)\n |-- Country: string (nullable = true)\n\n+-------+------+-------+\n|Product|Amount|Country|\n+-------+------+-------+\n|Banana |1000  |USA    |\n|Carrots|1500  |USA    |\n|Beans  |1600  |USA    |\n|Orange |2000  |USA    |\n|Orange |2000  |USA    |\n|Banana |400   |China  |\n|Carrots|1200  |China  |\n|Beans  |1500  |China  |\n|Orange |4000  |China  |\n|Banana |2000  |Canada |\n|Carrots|2000  |Canada |\n|Beans  |2000  |Mexico |\n+-------+------+-------+\n\nroot\n |-- Product: string (nullable = true)\n |-- Canada: long (nullable = true)\n |-- China: long (nullable = true)\n |-- Mexico: long (nullable = true)\n |-- USA: long (nullable = true)\n\n+-------+------+-----+------+----+\n|Product|Canada|China|Mexico|USA |\n+-------+------+-----+------+----+\n|Orange |null  |4000 |null  |4000|\n|Beans  |null  |1500 |2000  |1600|\n|Banana |2000  |400  |null  |1000|\n|Carrots|2000  |1200 |null  |1500|\n+-------+------+-----+------+----+\n\nroot\n |-- Product: string (nullable = true)\n |-- Canada: long (nullable = true)\n |-- China: long (nullable = true)\n |-- Mexico: long (nullable = true)\n |-- USA: long (nullable = true)\n\n+-------+------+-----+------+----+\n|Product|Canada|China|Mexico|USA |\n+-------+------+-----+------+----+\n|Orange |null  |4000 |null  |4000|\n|Beans  |null  |1500 |2000  |1600|\n|Banana |2000  |400  |null  |1000|\n|Carrots|2000  |1200 |null  |1500|\n+-------+------+-----+------+----+\n\n+-------+-------+-----+\n|Product|Country|Total|\n+-------+-------+-----+\n|Orange |China  |4000 |\n|Beans  |China  |1500 |\n|Beans  |Mexico |2000 |\n|Banana |Canada |2000 |\n|Banana |China  |400  |\n|Carrots|Canada |2000 |\n|Carrots|China  |1200 |\n+-------+-------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndept = [(\"Finance\",10), \\\n    (\"Marketing\",20), \\\n    (\"Sales\",30), \\\n    (\"IT\",40) \\\n  ]\n\nrdd=spark.sparkContext.parallelize(dept)\nprint(rdd)\ndataColl=rdd.collect()\n\nfor row in dataColl:\n    print(row[0] + \",\" +str(row[1]))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"2f122144-94c3-4e14-83cf-8034aca39d93","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["ParallelCollectionRDD[1405] at readRDDFromInputStream at PythonRDD.scala:435\nFinance,10\nMarketing,20\nSales,30\nIT,40\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndata = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),\n        (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),\n        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),\n        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)]\n\ncolumns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\npysparkDF = spark.createDataFrame(data = data, schema = columns)\npysparkDF.printSchema()\npysparkDF.show(truncate=False)\n\npandasDF = pysparkDF.toPandas()\nprint(pandasDF)\n\n# Nested structure elements\nfrom pyspark.sql.types import StructType, StructField, StringType,IntegerType\ndataStruct = [((\"James\",\"\",\"Smith\"),\"36636\",\"M\",\"3000\"), \\\n      ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",\"4000\"), \\\n      ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",\"4000\"), \\\n      ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",\"4000\"), \\\n      ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",\"-1\") \\\n]\n\nschemaStruct = StructType([\n        StructField('name', StructType([\n             StructField('firstname', StringType(), True),\n             StructField('middlename', StringType(), True),\n             StructField('lastname', StringType(), True)\n             ])),\n          StructField('dob', StringType(), True),\n         StructField('gender', StringType(), True),\n         StructField('salary', StringType(), True)\n         ])\n\n\ndf = spark.createDataFrame(data=dataStruct, schema = schemaStruct)\ndf.printSchema()\ndf.show(truncate=False)\n\npandasDF2 = df.toPandas()\nprint(pandasDF2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e5bb0c1c-ef7a-41fb-9ba6-700378794980","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- first_name: string (nullable = true)\n |-- middle_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+----------+-----------+---------+-----+------+------+\n|first_name|middle_name|last_name|dob  |gender|salary|\n+----------+-----------+---------+-----+------+------+\n|James     |           |Smith    |36636|M     |60000 |\n|Michael   |Rose       |         |40288|M     |70000 |\n|Robert    |           |Williams |42114|      |400000|\n|Maria     |Anne       |Jones    |39192|F     |500000|\n|Jen       |Mary       |Brown    |     |F     |0     |\n+----------+-----------+---------+-----+------+------+\n\n  first_name middle_name last_name    dob gender  salary\n0      James                 Smith  36636      M   60000\n1    Michael        Rose            40288      M   70000\n2     Robert              Williams  42114         400000\n3      Maria        Anne     Jones  39192      F  500000\n4        Jen        Mary     Brown             F       0\nroot\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: string (nullable = true)\n\n+--------------------+-----+------+------+\n|name                |dob  |gender|salary|\n+--------------------+-----+------+------+\n|{James, , Smith}    |36636|M     |3000  |\n|{Michael, Rose, }   |40288|M     |4000  |\n|{Robert, , Williams}|42114|M     |4000  |\n|{Maria, Anne, Jones}|39192|F     |4000  |\n|{Jen, Mary, Brown}  |     |F     |-1    |\n+--------------------+-----+------+------+\n\n                                                name    dob gender salary\n0  {'firstname': 'James', 'middlename': '', 'last...  36636      M   3000\n1  {'firstname': 'Michael', 'middlename': 'Rose',...  40288      M   4000\n2  {'firstname': 'Robert', 'middlename': '', 'las...  42114      M   4000\n3  {'firstname': 'Maria', 'middlename': 'Anne', '...  39192      F   4000\n4  {'firstname': 'Jen', 'middlename': 'Mary', 'la...             F     -1\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\n# Create SparkSession\nspark = SparkSession.builder \\\n          .appName('SparkByExamples.com') \\\n          .getOrCreate()\n          \n\ndata = [(1,10),(2,20),(3,10),(4,20),(5,10),\n    (6,30),(7,50),(8,50),(9,50),(10,30),\n    (11,10),(12,10),(13,40),(14,40),(15,40),\n    (16,40),(17,50),(18,10),(19,40),(20,40)\n  ]\n\ndf=spark.createDataFrame(data,[\"id\",\"value\"])\n\ndf.repartition(3,\"value\").explain(True)        \ndf.repartition(\"value\") \\\n  .write.option(\"header\",True) \\\n  .mode(\"overwrite\") \\\n  .csv(\"range-partition\")\n\ndf.repartitionByRange(\"value\").explain(True)\ndf.repartitionByRange(3,\"value\").explain(True)\n\ndf.repartitionByRange(3,\"value\") \\\n  .write.option(\"header\",True) \\\n  .mode(\"overwrite\") \\\n  .csv(\"range-partition-count\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c12300cd-e530-40df-a53c-63d21e47d737","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["== Parsed Logical Plan ==\n'RepartitionByExpression ['value], 3\n+- LogicalRDD [id#11083L, value#11084L], false\n\n== Analyzed Logical Plan ==\nid: bigint, value: bigint\nRepartitionByExpression [value#11084L], 3\n+- LogicalRDD [id#11083L, value#11084L], false\n\n== Optimized Logical Plan ==\nRepartitionByExpression [value#11084L], 3\n+- LogicalRDD [id#11083L, value#11084L], false\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Exchange hashpartitioning(value#11084L, 3), REPARTITION_BY_NUM, [plan_id=14903]\n   +- Scan ExistingRDD[id#11083L,value#11084L]\n\n== Parsed Logical Plan ==\n'RepartitionByExpression ['value ASC NULLS FIRST]\n+- LogicalRDD [id#11083L, value#11084L], false\n\n== Analyzed Logical Plan ==\nid: bigint, value: bigint\nRepartitionByExpression [value#11084L ASC NULLS FIRST]\n+- LogicalRDD [id#11083L, value#11084L], false\n\n== Optimized Logical Plan ==\nRepartitionByExpression [value#11084L ASC NULLS FIRST]\n+- LogicalRDD [id#11083L, value#11084L], false\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Exchange rangepartitioning(value#11084L ASC NULLS FIRST, 200), REPARTITION_BY_COL, [plan_id=14974]\n   +- Scan ExistingRDD[id#11083L,value#11084L]\n\n== Parsed Logical Plan ==\n'RepartitionByExpression ['value ASC NULLS FIRST], 3\n+- LogicalRDD [id#11083L, value#11084L], false\n\n== Analyzed Logical Plan ==\nid: bigint, value: bigint\nRepartitionByExpression [value#11084L ASC NULLS FIRST], 3\n+- LogicalRDD [id#11083L, value#11084L], false\n\n== Optimized Logical Plan ==\nRepartitionByExpression [value#11084L ASC NULLS FIRST], 3\n+- LogicalRDD [id#11083L, value#11084L], false\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Exchange rangepartitioning(value#11084L ASC NULLS FIRST, 3), REPARTITION_BY_NUM, [plan_id=14998]\n   +- Scan ExistingRDD[id#11083L,value#11084L]\n\n"]}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Spark_functions_Notebook 2023-06-11 17_59_52 (1)","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
